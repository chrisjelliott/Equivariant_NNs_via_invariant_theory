{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZTKnj+/Ss+BytOqU88KAL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisjelliott/Equivariant_NNs_via_invariant_theory/blob/main/Equivariant_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Equivariant Neural Networks for General Representations**\n",
        "\n",
        "In this notebook I'm going to implement some neural network models based on \"E(n) Equivariant Graph Neural Networks'' by Satorras, Hoogeboom and Welling https://arxiv.org/pdf/2102.09844 . I'll describe a generalization to the following situation.\n",
        "\n",
        "We'll describe an equivariant graph neural network associated to a graph $\\Gamma = (V,E)$ and a Lie group $G$.  So associated to each vertex $v_i$ of the graph we will have a variable $h_i \\in W_V$ where $W_V$ is a linear $G$-representation, and to each edge $e_{ij}$ we will have a variable $a_{ij} \\in W_E$ where $W_E$ is a linear $G$-representation.  Our model will learn $G$-equivariant functions with output encoded similarly by a graph for some new representations $W_V^{\\mathrm{out}}, W_E^{\\mathrm{out}}$.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "In the paper of Satorras et al the vertex representation takes the form $\\mathbb R^n \\times W$ where $G=E(n)$ acts by isometries on the first factor and trivially on the second factor.  The isometry action is affine, not linear, but we can turn it into a linear $G$-action on $\\mathbb R^{n+1}$."
      ],
      "metadata": {
        "id": "En7h3sSID5N7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Equivariant Layer Structure\n",
        "\n",
        "Let's describe a single layer using a generalization of Satorras et al's equivariant message passing layer.  Our approach is to take as input a suitable set of $G$-equivariant polynomial functions, and build functions from linear combinations of these functions.\n",
        "\n",
        "We'll start with the message function, associated to a single edge $e_{ij}$ in the graph.  We will build equivariant functions\n",
        "$$F \\in M_I = (\\mathbb R[W_V^2 \\times W_E] \\otimes W_I)^G$$\n",
        "where $W_I$ is some intermediate $G$-representation.  This $M_I$ is a module over the algebra of invariant functions $R_I =  \\mathbb R[W_V^2 \\times W_E]^G$.\n",
        "\n",
        "We make the assumption that $R_I$ is a fininitely generated algebra with basis $f_1, \\ldots, f_n$ and that $M_I$ is finitely generated as an $R_I$-module with basis $\\mu_1, \\ldots, \\mu_N$ (for instance, this is guaranteed if $G$ is reductive).  We consider the following set of equivariant functions:\n",
        "$$\\{F^\\alpha \\colon \\alpha \\in A\\} = \\{\\mu_l\\} \\cup \\{f_k \\cdot \\mu_l\\}.$$\n",
        "Note that this set might not be linearly independent, there may be linear relations (syzygys) between the elements, leading to some potential redundancy in functions represented as linear combinations.\n",
        "\n",
        "The interpretation here is that we are generalizing the set of affine functions (sums of linear functions and constant functions) between vector spaces by including lowest order and next-to-lowest order generators.\n",
        "\n",
        "So we can now define the possible message functions.  These will take the form\n",
        "$$m_{ij} = \\sigma_I \\left(\\sum_{\\alpha \\in A} a_\\alpha f^\\alpha(h_i, h_j, a_{ij})  \\right)$$\n",
        "for some learnable coefficients $a_\\alpha$, and some pointwise activation function $\\sigma_I$.\n",
        "\n",
        "We can use these message functions to update the vertex and edge variables.  We will again construct sets of invariant functions\n",
        "\\begin{align}\n",
        "g^\\beta &\\in (\\mathbb R[W_V \\times W_I] \\otimes W_V^{\\mathrm{out}})^G \\\\\n",
        "k^\\gamma &\\in (\\mathbb R[W_E \\times W_I] \\otimes W_E^{\\mathrm{out}})^G\n",
        "\\end{align}\n",
        "in exactly the same way.  If we choose another activation function $\\sigma$ then the updated vertex and edge variables are given as follows:\n",
        "\\begin{align}\n",
        "h_i^{\\mathrm{out}} &= \\sigma \\left(b_\\beta g^\\beta\\left(h_i, \\sum_{v_j \\in N(v_i)} m_{ij} \\right) \\right) \\\\\n",
        "a_{ij}^{\\mathrm{out}} &= \\sigma \\left(c_\\gamma k^\\gamma\\left(a_{ij}, \\sum_{v_\\ell \\in N(v_i)} m_{i\\ell} + \\sum_{v_\\ell \\in N(v_j)} m_{j\\ell}  \\right) \\right)\n",
        "\\end{align}\n",
        "where again $b_\\beta, c_\\gamma$ are learnable weights, and where we write $N(v_i)$ for the neighborhood of vertex $v_i$ in the graph $\\Gamma$."
      ],
      "metadata": {
        "id": "umLhZLf2QfPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example 1:\n",
        "\n",
        "We can check that if $G$ is trivial then we recover a usual graph neural network architecture.  Indeed, in the trivial case, when we study functions $W_1 \\to W_2$, the generators in our model are given as follows.\n",
        "\n",
        "*   Generators of $W_2$ as an $\\mathbb R[W_1]$-module -- basis vectors $e^{(2)}_i$\n",
        "*   The product of generators of $W_2$ with algebra generators of $\\mathbb R[W_1]$ -- tensors of the form $(e^{(1)}_j)^* \\otimes e^{(2)}_i$.  In other words, matrix elements in $W_1^* \\otimes W_2$.\n",
        "\n",
        "Linear combinations of the first type of element generate constant functions $W_1 \\to W_2$, and linear combinations of the second type of element generate linear functions $W_1 \\to W_2$.  So altogether when we take arbitrary linear combinations in our model we are just considering the set of affine functions."
      ],
      "metadata": {
        "id": "bEeHFM9yiUmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example 2:\n",
        "\n",
        "Let's consider the example of Satorras, Hoogeboom and Welling.  So let $G = E(n)$, let $W_E$ be a trivial representation, and let $W_V = \\mathbb R^{n+1} \\times U$ where $U$ is again a trivial representation, and $E(n)$ acts on $\\mathbb R^{n+1} = \\mathbb R^n \\times \\mathbb R$ by affine transformations:\n",
        "$$(C,b) \\cdot (v, t) = (Cv + tb, t).$$\n",
        "We obtain the usual affine action on $\\mathbb R^n$ by restricting to the hyperplane $t=1$, and we will restrict attention to $E(n)$-equivariant functions that preserve this hyperplane.\n",
        "\n",
        "**One Spatial Input**\n",
        "\n",
        "Let us start by analyzing the equivariant functions of the form\n",
        "$$F \\colon \\mathbb R^{n+1} \\times U_1 \\to \\mathbb R^{n+1} \\times U_2$$\n",
        "where $U_1, U_2$ are trivial representations.  So according to our procedure we will need to compute algebra generators for $$A = \\mathbb R[\\mathbb R^{n+1} \\times U_1]^{E(n)}$$ and module generators for $$M = (\\mathbb R[\\mathbb R^{n+1} \\times U_1] \\otimes (\\mathbb R^{n+1} \\times U_2))^{E(n)}.$$  In each case we will restrict to those functions that preserve the $(n+1)^{\\text{st}}$ coordinate in $\\mathbb R^{n+1}$.\n",
        "\n",
        "In the first case $A \\cong \\mathbb R[\\mathbb R^{n+1}]^{E(n)} \\otimes \\mathbb R[U_1]$, and the only generators that preserve the final coordinate are constant in the first factor, so we have generators associated to basis vectors in $U_1$.\n",
        "\n",
        "In the latter case, $M$ is generated as a module by constant functions to $U_2$ together with the identity function $\\mathbb R^{n+1} \\to \\mathbb R^{n+1}$.  So, altogether, the set of zeroth and first order generating functions can be identified with $$\\{f^\\alpha\\} \\cong \\langle \\mathrm{id}\\rangle \\oplus U_2 \\oplus (U_1^* \\otimes \\langle\\mathrm{id}\\rangle) \\oplus (U_1^* \\otimes U_2).$$\n",
        "\n",
        "**Two Spatial Inputs**\n",
        "\n",
        "Finally, associated to the intermediate term we have a variant of this computation.  We need to compute algebra generators for $$A = \\mathbb R[(\\mathbb R^{n+1})^2 \\times U_1]^{E(n)}$$ and module generators for $$M = (\\mathbb R[(\\mathbb R^{n+1})^2 \\times U_1] \\otimes (\\mathbb R^{n+1} \\times U_2))^{E(n)},$$\n",
        "again preserving the hyperplane where the final coordinate in $\\mathbb R^{n+1}$ is equal to one.\n",
        "\n",
        "Let us use the notation $((x_1, t_1), (x_2, t_2)) \\in (\\mathbb R^{n+1})^2$. The algebra $A$ now has generators associated to basis vectors in $U_1$, but in addition we have a quadratic generator of the form $\\|x_1 - x_2\\|^2$.  The module $M$ still has module generators associated to constant functions to $U_2$, but rather than the identity there are now two additional generators associated to the projections $\\pi_1, \\pi_2$ onto the two factors of $(\\mathbb R^{n+1})^2$.  So now, altogether, the set of zeroth and first order generating functions can be identified with\n",
        "$$\\{f^\\alpha\\} \\cong \\langle \\pi_1, \\pi_2 \\rangle \\oplus U_2 \\oplus (U_1^* \\otimes \\langle \\pi_1, \\pi_2 \\rangle) \\oplus (\\langle \\|x_1 - x_2 \\|^2 \\rangle \\otimes \\langle \\pi_1, \\pi_2 \\rangle) \\oplus (U_1^* \\otimes U_2) \\oplus (U_1^* \\otimes \\langle \\|x_1 - x_2 \\|^2 \\rangle).$$\n",
        "In order to preserve the hyperplane we will need to restrict attention to those linear combinations $a_1 \\pi_1 + a_2 \\pi_2$ where $a_1 + a_2 = 1$.\n",
        "\n"
      ],
      "metadata": {
        "id": "OhTOk5LEgo0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation\n",
        "\n",
        "Let's go ahead and implement the equivariant graph convolution layer following this procedure.  Note that Satorras et al actually allow their message passing and vertex update terms to contain two layers: linear -> activation -> linear -> activation (where the second activation may be constant).  I will include this behaviour as an option if desired so that we can compare the results with one and two layers."
      ],
      "metadata": {
        "id": "RqdbXUj-UuYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EGCL(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_vertices: int,\n",
        "        adj_matrix: torch.Tensor,\n",
        "        vertex_inputs: int,\n",
        "        edge_inputs: int,\n",
        "        vertex_outputs: int,\n",
        "        edge_outputs: int,\n",
        "        inter_vars: int,\n",
        "        inter_invt_funs: list,\n",
        "        vertex_invt_funs: list,\n",
        "        edge_invt_funs: list,\n",
        "        inter_activation: callable,\n",
        "        vertex_activation: callable,\n",
        "        edge_activation: callable,\n",
        "        double_layer: bool = False,\n",
        "        inter_vars_2: int = None,\n",
        "        vertex_hidden: int = None,\n",
        "        edge_hidden: int = None,\n",
        "        inter_activation_2: callable = None,\n",
        "        vertex_activation_2: callable = None,\n",
        "        edge_activation_2: callable = None,\n",
        "        is_affine: bool = False,\n",
        "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Equivariant Graph Convolution Layer with optional double layer processing.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        num_vertices : int\n",
        "            Number of vertices in the graph\n",
        "        adj_matrix : torch.Tensor\n",
        "            Adjacency matrix of the graph (shape: [num_vertices, num_vertices])\n",
        "        vertex_inputs : int\n",
        "            Dimension of vertex input features\n",
        "        edge_inputs : int\n",
        "            Dimension of edge input features\n",
        "        vertex_outputs : int\n",
        "            Dimension of vertex output features\n",
        "        edge_outputs : int\n",
        "            Dimension of edge output features\n",
        "        inter_vars : int\n",
        "            Dimension of intermediate message features\n",
        "        inter_invt_funs : list\n",
        "            List of invariant functions for message passing\n",
        "        vertex_invt_funs : list\n",
        "            List of invariant functions for vertex updates\n",
        "        edge_invt_funs : list\n",
        "            List of invariant functions for edge updates\n",
        "        inter_activation : callable\n",
        "            Activation function for intermediate computations\n",
        "        vertex_activation : callable\n",
        "            Activation function for vertex updates\n",
        "        edge_activation : callable\n",
        "            Activation function for edge updates\n",
        "        double_layer : bool\n",
        "            Whether to use double layer processing (default: False)\n",
        "        inter_vars_2 : int\n",
        "            Dimension of second intermediate layer (default: same as inter_vars)\n",
        "        vertex_hidden : int\n",
        "            Dimension of hidden layer in vertex update (default: same as vertex_outputs)\n",
        "        edge_hidden : int\n",
        "            Dimension of hidden layer in edge update (default: same as edge_outputs)\n",
        "        inter_activation_2 : callable\n",
        "            Second activation for message passing (default: same as inter_activation)\n",
        "        vertex_activation_2 : callable\n",
        "            Second activation for vertex update (default: same as vertex_activation)\n",
        "        edge_activation_2 : callable\n",
        "            Second activation for edge update (default: same as edge_activation)\n",
        "        is_affine : bool\n",
        "            Whether to normalize outputs to preserve affine transformations\n",
        "        device : str\n",
        "            Device to run computations on\n",
        "        \"\"\"\n",
        "        super(EGCL, self).__init__()\n",
        "\n",
        "        # Validate inputs\n",
        "        if not isinstance(adj_matrix, torch.Tensor):\n",
        "            adj_matrix = torch.tensor(adj_matrix, dtype=torch.float32)\n",
        "        if adj_matrix.shape != (num_vertices, num_vertices):\n",
        "            raise ValueError(f\"adj_matrix shape {adj_matrix.shape} doesn't match num_vertices {num_vertices}\")\n",
        "\n",
        "        # Store basic parameters\n",
        "        self.device = device\n",
        "        self.num_vertices = num_vertices\n",
        "        self.adj_matrix = adj_matrix.to(device)\n",
        "        self.vertex_inputs = vertex_inputs\n",
        "        self.edge_inputs = edge_inputs\n",
        "        self.vertex_outputs = vertex_outputs\n",
        "        self.edge_outputs = edge_outputs\n",
        "        self.inter_vars = inter_vars\n",
        "        self.inter_invt_funs = inter_invt_funs\n",
        "        self.vertex_invt_funs = vertex_invt_funs\n",
        "        self.edge_invt_funs = edge_invt_funs\n",
        "        self.inter_activation = inter_activation\n",
        "        self.vertex_activation = vertex_activation\n",
        "        self.edge_activation = edge_activation\n",
        "        self.is_affine = is_affine\n",
        "\n",
        "        self.num_inter_invts = len(inter_invt_funs)\n",
        "        self.num_vertex_invts = len(vertex_invt_funs)\n",
        "        self.num_edge_invts = len(edge_invt_funs)\n",
        "\n",
        "        # Initialize first layer weights\n",
        "        n_in = 2*vertex_inputs + edge_inputs  # Total input dimension\n",
        "        n_out = inter_vars                    # Output dimension\n",
        "        std = torch.sqrt(torch.tensor(6.0/(n_in + n_out)))\n",
        "\n",
        "        self.inter_weights = nn.Parameter(\n",
        "            torch.randn(self.num_inter_invts, device=device) * std\n",
        "        )\n",
        "\n",
        "        # For vertex update weights\n",
        "        n_in = vertex_inputs + inter_vars     # Input vertex features + message features\n",
        "        n_out = vertex_outputs\n",
        "        std = torch.sqrt(torch.tensor(6.0/(n_in + n_out)))\n",
        "\n",
        "        self.vertex_weights = nn.Parameter(\n",
        "            torch.randn(self.num_vertex_invts, device=device) * std\n",
        "        )\n",
        "\n",
        "        # For edge update weights\n",
        "        n_in = edge_inputs + 2*inter_vars     # Edge features + sum of messages\n",
        "        n_out = edge_outputs\n",
        "        std = torch.sqrt(torch.tensor(6.0/(n_in + n_out)))\n",
        "\n",
        "        self.edge_weights = nn.Parameter(\n",
        "            torch.randn(self.num_edge_invts, device=device) * std\n",
        "        )\n",
        "\n",
        "        # Handle double layer parameters\n",
        "        self.double_layer = double_layer\n",
        "        if double_layer:\n",
        "            self.inter_vars_2 = inter_vars_2 if inter_vars_2 is not None else inter_vars\n",
        "            self.vertex_hidden = vertex_hidden if vertex_hidden is not None else vertex_outputs\n",
        "            self.edge_hidden = edge_hidden if edge_hidden is not None else edge_outputs\n",
        "            self.inter_activation_2 = inter_activation_2 if inter_activation_2 is not None else inter_activation\n",
        "            self.vertex_activation_2 = vertex_activation_2 if vertex_activation_2 is not None else vertex_activation\n",
        "            self.edge_activation_2 = edge_activation_2 if edge_activation_2 is not None else edge_activation\n",
        "\n",
        "            # Initialize second layer weights\n",
        "            n_in = inter_vars  # Total input dimension\n",
        "            n_out = inter_vars_2       # Output dimension\n",
        "            std = torch.sqrt(torch.tensor(6.0/(n_in + n_out)))\n",
        "\n",
        "            self.inter_weights_2 = nn.Parameter(\n",
        "                torch.randn(self.num_inter_vars_2, device=device) * std\n",
        "            )\n",
        "\n",
        "            # For vertex update weights\n",
        "            n_in = vertex_hidden   # Input vertex features + message features\n",
        "            n_out = vertex_outputs\n",
        "            std = torch.sqrt(torch.tensor(6.0/(n_in + n_out)))\n",
        "\n",
        "            self.vertex_weights_2 = nn.Parameter(\n",
        "                torch.randn(self.num_vertex_outputs, device=device) * std\n",
        "            )\n",
        "\n",
        "            # For edge update weights\n",
        "            n_in = edge_hidden    # Edge features + sum of messages\n",
        "            n_out = edge_outputs\n",
        "            std = torch.sqrt(torch.tensor(6.0/(n_in + n_out)))\n",
        "\n",
        "            self.edge_weights_2 = nn.Parameter(\n",
        "                torch.randn(self.num_edge_outputs, device=device) * std\n",
        "            )\n",
        "\n",
        "        # Pre-compute neighborhoods\n",
        "        self.neighborhoods = self._compute_neighborhoods()\n",
        "\n",
        "    def _compute_neighborhoods(self):\n",
        "        \"\"\"Pre-compute neighborhoods for each vertex.\"\"\"\n",
        "        neighborhoods = {}\n",
        "        for i in range(self.num_vertices):\n",
        "            neighborhoods[i] = torch.nonzero(self.adj_matrix[i], as_tuple=False).squeeze(1)\n",
        "        return neighborhoods\n",
        "\n",
        "    def _normalize_if_affine(self, tensor):\n",
        "        \"\"\"Normalize tensor if is_affine is True and tensor is non-zero.\"\"\"\n",
        "        if self.is_affine and torch.norm(tensor) > 1e-8:\n",
        "            return F.normalize(tensor, dim=-1)\n",
        "        return tensor\n",
        "\n",
        "    def intermediate_term(self, h_i, h_j, a_ij):\n",
        "        \"\"\"Compute message from vertex i to vertex j.\"\"\"\n",
        "        try:\n",
        "            # First layer\n",
        "            invt_outputs = torch.stack([\n",
        "                torch.as_tensor(f(h_i, h_j, a_ij), device=self.device)\n",
        "                for f in self.inter_invt_funs\n",
        "            ])\n",
        "            message = torch.matmul(self.inter_weights, invt_outputs)\n",
        "            message = self._normalize_if_affine(message)\n",
        "            message = self.inter_activation(message)\n",
        "\n",
        "            if not self.double_layer:\n",
        "                return message\n",
        "\n",
        "            # Second layer\n",
        "            message = torch.matmul(self.inter_weights_2, message)\n",
        "            message = self._normalize_if_affine(message)\n",
        "            return self.inter_activation_2(message)\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            raise RuntimeError(f\"Error in intermediate_term: {str(e)}\")\n",
        "\n",
        "    def vertex_update(self, h, m):\n",
        "        \"\"\"Update vertex features.\"\"\"\n",
        "        try:\n",
        "            # First layer\n",
        "            invt_outputs = torch.stack([\n",
        "                torch.as_tensor(f(h, m), device=self.device)\n",
        "                for f in self.vertex_invt_funs\n",
        "            ])\n",
        "            update = torch.matmul(self.vertex_weights, invt_outputs)\n",
        "            update = self._normalize_if_affine(update)\n",
        "            update = self.vertex_activation(update)\n",
        "\n",
        "            if not self.double_layer:\n",
        "                return update\n",
        "\n",
        "            # Second layer\n",
        "            update = torch.matmul(self.vertex_weights_2, update)\n",
        "            update = self._normalize_if_affine(update)\n",
        "            return self.vertex_activation_2(update)\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            raise RuntimeError(f\"Error in vertex_update: {str(e)}\")\n",
        "\n",
        "    def edge_update(self, a, m):\n",
        "        \"\"\"Update edge features.\"\"\"\n",
        "        try:\n",
        "            # First layer\n",
        "            invt_outputs = torch.stack([\n",
        "                torch.as_tensor(f(a, m), device=self.device)\n",
        "                for f in self.edge_invt_funs\n",
        "            ])\n",
        "            update = torch.matmul(self.edge_weights, invt_outputs)\n",
        "            update = self._normalize_if_affine(update)\n",
        "            update = self.edge_activation(update)\n",
        "\n",
        "            if not self.double_layer:\n",
        "                return update\n",
        "\n",
        "            # Second layer\n",
        "            update = torch.matmul(self.edge_weights_2, update)\n",
        "            update = self._normalize_if_affine(update)\n",
        "            return self.edge_activation_2(update)\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            raise RuntimeError(f\"Error in edge_update: {str(e)}\")\n",
        "\n",
        "    def forward(self, h_graph: torch.Tensor, a_graph: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass of the layer.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        h_graph : torch.Tensor\n",
        "            Vertex features (shape: [num_vertices, vertex_inputs])\n",
        "        a_graph : torch.Tensor\n",
        "            Edge features (shape: [num_vertices, num_vertices, edge_inputs])\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple(torch.Tensor, torch.Tensor)\n",
        "            Updated vertex and edge features\n",
        "        \"\"\"\n",
        "        if h_graph.shape != (self.num_vertices, self.vertex_inputs):\n",
        "            raise ValueError(f\"h_graph shape {h_graph.shape} doesn't match expected shape\"\n",
        "                           f\" ({self.num_vertices}, {self.vertex_inputs})\")\n",
        "        if a_graph.shape != (self.num_vertices, self.num_vertices, self.edge_inputs):\n",
        "            raise ValueError(f\"a_graph shape {a_graph.shape} doesn't match expected shape\"\n",
        "                           f\" ({self.num_vertices}, {self.num_vertices}, {self.edge_inputs})\")\n",
        "\n",
        "        # Move inputs to correct device\n",
        "        h_graph = h_graph.to(self.device)\n",
        "        a_graph = a_graph.to(self.device)\n",
        "\n",
        "        # Initialize output tensors\n",
        "        h_graph_out = torch.zeros(\n",
        "            (self.num_vertices, self.vertex_outputs),\n",
        "            device=self.device\n",
        "        )\n",
        "        a_graph_out = torch.zeros(\n",
        "            (self.num_vertices, self.num_vertices, self.edge_outputs),\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Compute messages for all edges\n",
        "        messages = torch.zeros(\n",
        "            (self.num_vertices, self.num_vertices, self.inter_vars),\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Compute messages only for existing edges (where adj_matrix is 1)\n",
        "        edge_indices = torch.nonzero(self.adj_matrix, as_tuple=True)\n",
        "        for i, j in zip(*edge_indices):\n",
        "            messages[i, j] = self.intermediate_term(h_graph[i], h_graph[j], a_graph[i, j])\n",
        "\n",
        "        # Sum messages for each vertex\n",
        "        message_sums = torch.sum(\n",
        "            messages * self.adj_matrix.unsqueeze(-1),\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # Update vertices\n",
        "        for i in range(self.num_vertices):\n",
        "            h_graph_out[i] = self.vertex_update(h_graph[i], message_sums[i])\n",
        "\n",
        "        # Update edges\n",
        "        for i, j in zip(*edge_indices):\n",
        "            a_graph_out[i, j] = self.edge_update(\n",
        "                a_graph[i, j],\n",
        "                message_sums[i] + message_sums[j]\n",
        "            )\n",
        "\n",
        "        return h_graph_out, a_graph_out"
      ],
      "metadata": {
        "id": "yTNOgXMePV4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by testing the layer with an example.  I'll let $G = E(3)$ and use the fundamental representation $\\mathbb R^4$ for the vertex space $W_V$ and intermediate space and the trivial representation $\\mathbb R$ for the edge space $W_E$.  I'll just list the invariant functions defined individually."
      ],
      "metadata": {
        "id": "VAxY7_T5n2uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_vertices = 5\n",
        "adj_matrix = torch.randint(0,2,(num_vertices, num_vertices))\n",
        "adj_matrix = (adj_matrix + adj_matrix.T) / 2\n",
        "\n",
        "def inter_invt_fun1(h_i, h_j, a_ij):\n",
        "    return h_i\n",
        "\n",
        "def inter_invt_fun2(h_i, h_j, a_ij):\n",
        "    return h_j\n",
        "\n",
        "def inter_invt_fun3(h_i, h_j, a_ij):\n",
        "    return a_ij * h_i\n",
        "\n",
        "def inter_invt_fun4(h_i, h_j, a_ij):\n",
        "    return a_ij * h_j\n",
        "\n",
        "def inter_invt_fun5(h_i, h_j, a_ij):\n",
        "    return h_i * torch.linalg.norm(h_i - h_j) ** 2\n",
        "\n",
        "def inter_invt_fun6(h_i, h_j, a_ij):\n",
        "    return h_j * torch.linalg.norm(h_i - h_j) ** 2\n",
        "\n",
        "def vertex_invt_fun1(h, m):\n",
        "    return h\n",
        "\n",
        "def vertex_invt_fun2(h, m):\n",
        "    return m\n",
        "\n",
        "def vertex_invt_fun3(h, m):\n",
        "    return h * torch.linalg.norm(h - m) ** 2\n",
        "\n",
        "def vertex_invt_fun4(h, m):\n",
        "    return m * torch.linalg.norm(h - m) ** 2\n",
        "\n",
        "\n",
        "def edge_invt_fun(a, m):\n",
        "    return a\n",
        "\n",
        "inter_invt_funs = [\n",
        "    inter_invt_fun1,\n",
        "    inter_invt_fun2,\n",
        "    inter_invt_fun3,\n",
        "    inter_invt_fun4,\n",
        "    inter_invt_fun5,\n",
        "    inter_invt_fun6\n",
        "]\n",
        "\n",
        "vertex_invt_funs = [\n",
        "    vertex_invt_fun1,\n",
        "    vertex_invt_fun2,\n",
        "    vertex_invt_fun3,\n",
        "    vertex_invt_fun4\n",
        "]\n",
        "\n",
        "edge_invt_funs = [edge_invt_fun]"
      ],
      "metadata": {
        "id": "N53llv26dhSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = EGCL(\n",
        "    num_vertices=num_vertices,\n",
        "    adj_matrix=adj_matrix,\n",
        "    vertex_inputs=4,    # R^4 for vertex features\n",
        "    edge_inputs=1,      # R^1 for edge features (trivial rep)\n",
        "    vertex_outputs=4,   # R^4 output\n",
        "    edge_outputs=1,     # R^1 output\n",
        "    inter_vars=4,       # R^4 for intermediate representation\n",
        "    inter_invt_funs=inter_invt_funs,\n",
        "    vertex_invt_funs=vertex_invt_funs,\n",
        "    edge_invt_funs=edge_invt_funs,\n",
        "    inter_activation=torch.nn.ReLU(),\n",
        "    vertex_activation=torch.nn.ReLU(),\n",
        "    edge_activation=torch.nn.ReLU(),\n",
        "    is_affine=True     # We are using an affine group so should rescale to preserve a hyperplane\n",
        ")\n",
        "\n",
        "# Create some example input data\n",
        "h_graph = torch.randn(num_vertices, 4)  # Random vertex features in R^4\n",
        "a_graph = torch.randn(num_vertices, num_vertices, 1)  # Random edge features\n",
        "\n",
        "# Forward pass\n",
        "h_out, a_out = layer(h_graph, a_graph)\n",
        "\n",
        "print(f\"Input vertex features shape: {h_graph.shape}\")\n",
        "print(f\"Output vertex features shape: {h_out.shape}\")\n",
        "print(f\"Input edge features shape: {a_graph.shape}\")\n",
        "print(f\"Output edge features shape: {a_out.shape}\")\n",
        "\n",
        "# Print statistics before and after activation\n",
        "def print_activation_stats(tensor, name):\n",
        "    print(f\"\\n{name} statistics:\")\n",
        "    print(f\"Min: {tensor.min():.4f}\")\n",
        "    print(f\"Max: {tensor.max():.4f}\")\n",
        "    print(f\"Mean: {tensor.mean():.4f}\")\n",
        "    print(f\"Std: {tensor.std():.4f}\")\n",
        "\n",
        "# Get pre-activation values (you'd need to modify the layer to expose these)\n",
        "print_activation_stats(h_out, \"Vertex outputs\")\n",
        "print_activation_stats(a_out, \"Edge outputs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPpoPLEfvDYy",
        "outputId": "a82e090f-1d53-498d-df58-e399592e8a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input vertex features shape: torch.Size([5, 4])\n",
            "Output vertex features shape: torch.Size([5, 4])\n",
            "Input edge features shape: torch.Size([5, 5, 1])\n",
            "Output edge features shape: torch.Size([5, 5, 1])\n",
            "\n",
            "Vertex outputs statistics:\n",
            "Min: 0.0000\n",
            "Max: 0.9097\n",
            "Mean: 0.2938\n",
            "Std: 0.2783\n",
            "\n",
            "Edge outputs statistics:\n",
            "Min: 0.0000\n",
            "Max: 1.0000\n",
            "Mean: 0.4800\n",
            "Std: 0.5099\n",
            "tensor([[-0.5459,  0.6279, -0.5070,  1.7881],\n",
            "        [-0.8125, -0.3071, -0.2378, -0.5380],\n",
            "        [ 1.5171,  0.9660,  0.1619,  1.3033],\n",
            "        [ 1.2586, -1.4620,  0.2673,  0.7281],\n",
            "        [-0.5803, -0.5267, -0.0819, -1.6034]])\n",
            "tensor([[0.0160, 0.4108, 0.0000, 0.9097],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0348],\n",
            "        [0.5629, 0.5036, 0.1853, 0.6287],\n",
            "        [0.4638, 0.6573, 0.1694, 0.5693],\n",
            "        [0.1966, 0.2954, 0.2721, 0.0000]], grad_fn=<CopySlices>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building $E(n)$ Equivariant Models\n",
        "\n",
        "The next step is to build a model.  I'll stick with the $E(n)$ example, and representations of the form $(\\mathbb R^{n+1})^a \\times \\mathbb R^b$ -- products of the fundamental and trivial representations.  I'll need a general method for constructing the sets of generating equivariant functions.\n",
        "\n",
        "Let's consider equivariant functions\n",
        "$$F \\colon (\\mathbb R^{n+1})^a \\times \\mathbb R^b \\to (\\mathbb R^{n+1})^c \\times \\mathbb R^d.$$\n",
        "The set of $\\mathrm O(n)$-equivariant functions we will wish to generate will consist of\n",
        "$$\\{x_{ij}, e_{kl}, \\langle x_p, x_q \\rangle x_{ij}, \\langle x_p, x_q \\rangle x_{ij}, e_r x_{ij}, e_r e_{kl}\\}$$\n",
        "where $x_{ij}, e_{kl}$ are the matrix element functions in the first and second factors respectively, and where $x_p, e_r$ are coordinate functions on the $p^\\text{th}$ factor of $(\\mathbb R^{n+1})^a$ and the $r^\\text{th}$ factor of $\\mathbb R^b$ respectively.  To be additionally translation equivariant we must restrict the inner product terms to those generated by $\\langle x_p, x_q \\rangle - \\langle x_p, x_{q'} \\rangle$, and the $x_{ij}$ terms to linear combinations with coefficients summing to one.\n"
      ],
      "metadata": {
        "id": "ziO65nhlVBrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def En_invariant_functions(dim, vector_in, scalar_in, vector_out, scalar_out):\n",
        "\n",
        "  invariants = []\n",
        "\n",
        "  for i in range(vector_in):\n",
        "    for j in range(vector_out):\n",
        "      invariants.append(lambda x, c: torch.cat(x[i]* torch.eye(vector_out)[j], torch.zeros(scalar_out)))\n",
        "      for r in range(scalar_in):\n",
        "        invariants.append(lambda x, c: torch.cat(c[r] * x[i]* torch.eye(vector_out)[j], torch.zeros(scalar_out)))\n",
        "      for p in range(vector_in):\n",
        "        for q in range(vector_in):\n",
        "          invariants.append(lambda x,c : torch.cat(torch.dot(x[p], x[q]) * x[i] * torch.eye(vector_out)[j], torch.zeros(scalar_out)))\n",
        "\n",
        "  for k in range(scalar_in):\n",
        "    for l in range(scalar_out):\n",
        "      invariants.append(lambda x, c: torch.cat(torch.zeros(dim * vector_out), c[k]* torch.eye(scalar_out)[l]))\n",
        "      for r in range(scalar_in):\n",
        "        invariants.append(lambda x, c: torch.cat(torch.zeros(dim * vector_out), c[k]* c[r] * torch.eye(scalar_out)[l]))\n",
        "      for p in range(vector_in):\n",
        "        for q in range(vector_in):\n",
        "          invariants.append(lambda x, c : torch.cat(torch.zeros(dim * vector_out), torch.dot(x[p], x[q]) * c[k] * torch.eye(scalar_out)[l], 0))\n",
        "\n",
        "  return torch.Tensor(invariants)\n",
        "\n",
        "def reshape_vertex_edge_to_scalar_vector(dim, in_1, vector_1, scalar_1, in_2, vector_2, scalar_2, in_3 = None, scalar_3 = 0, vector_3 = 0, is_message = False):\n",
        "\n",
        "  assert in_1.shape[0] == vector_1 * dim + scalar_1\n",
        "  assert in_2.shape[0] == vector_2 * dim + scalar_2\n",
        "  if is_message:\n",
        "    assert in_3.shape[0] == vector_3 * dim + scalar_3\n",
        "\n",
        "  if is_message:\n",
        "    vector_out = torch.cat(in_1[:dim*vector_1], in_2[:dim*vector_2], in_3[:dim*vector_3])\n",
        "    scalar_out = torch.cat(in_1[dim*vector_1:], in_2[dim*vector_2:], in_3[dim*vector_3:])\n",
        "  else:\n",
        "    vector_out = torch.cat(in_1[:dim*vector_1], in_2[:dim*vector_2])\n",
        "    scalar_out = torch.cat(in_1[dim*vector_1:], in_2[dim*vector_2:])\n",
        "\n",
        "  return vector_out, scalar_out\n",
        "\n",
        "def En_invariant_generator(dim, vector_in_1, scalar_in_1, vector_in_2, scalar_in_2, vector_out, scalar_out, vector_in_3 = 0, scalar_in_3 = 0):\n",
        "\n",
        "  invariants = []\n",
        "\n",
        "  if vector_in_3 == 0 and scalar_in_3 == 0:\n",
        "    invariant_to_reshape = En_invariant_functions(dim, vector_in_1 + vector_in_2, scalar_in_1 + scalar_in_2, vector_out, scalar_out)\n",
        "\n",
        "    for f in invariant_to_reshape:\n",
        "      invariants.append(lambda h, m: f(reshape_vertex_edge_to_scalar_vector(dim, h.shape[0], vector_in_1, scalar_in_1, m.shape[0], vector_in_2, scalar_in_2)))\n",
        "\n",
        "  else:\n",
        "    invariant_to_reshape = En_invariant_functions(dim, vector_in_1 + vector_in_2 + vector_in_3, scalar_in_1 + scalar_in_2 + scalar_in_3, vector_out, scalar_out)\n",
        "\n",
        "    for f in invariant_to_reshape:\n",
        "      invariants.append(lambda h_1, h_2, m: f(reshape_vertex_edge_to_scalar_vector(dim, h_1.shape[0], vector_in_1, scalar_in_1,\n",
        "                                                      h_2.shape[0], vector_in_2, scalar_in_2, m.shape[0], vector_in_3, scalar_in_3, is_message = True)))\n",
        "\n",
        "  return invariants\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MXmOt3l0yZw8"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}