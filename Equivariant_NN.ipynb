{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7i+VA4r2M+sCQSd7hEmTC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisjelliott/Equivariant_NNs_via_invariant_theory/blob/main/Equivariant_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Equivariant Neural Networks for General Representations**\n",
        "\n",
        "In this notebook I'm going to implement some neural network models based on \"E(n) Equivariant Graph Neural Networks'' by Satorras, Hoogeboom and Welling https://arxiv.org/pdf/2102.09844 . I'll describe a generalization to the following situation.\n",
        "\n",
        "We'll describe an equivariant graph neural network associated to a graph $\\Gamma = (V,E)$ and a Lie group $G$.  So associated to each vertex $v_i$ of the graph we will have a variable $h_i \\in W_V$ where $W_V$ is a linear $G$-representation, and to each edge $e_{ij}$ we will have a variable $a_{ij} \\in W_E$ where $W_E$ is a linear $G$-representation.  Our model will learn $G$-equivariant functions with output encoded similarly by a graph for some new representations $W_V^{\\mathrm{out}}, W_E^{\\mathrm{out}}$.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "In the paper of Satorras et al the vertex representation takes the form $\\mathbb R^n \\times W$ where $G=E(n)$ acts by isometries on the first factor and trivially on the second factor.  The isometry action is affine, not linear, but we can turn it into a linear $G$-action on $\\mathbb R^{n+1}$."
      ],
      "metadata": {
        "id": "En7h3sSID5N7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Equivariant Layer Structure\n",
        "\n",
        "Let's describe a single layer using a generalization of Satorras et al's equivariant message passing layer.  Our approach is to take as input a suitable set of $G$-equivariant polynomial functions, and build functions from linear combinations of these functions.\n",
        "\n",
        "We'll start with the message function, associated to a single edge $e_{ij}$ in the graph.  We will build equivariant functions\n",
        "$$F \\in M_I = (\\mathbb R[W_V^2 \\times W_E] \\otimes W_I)^G$$\n",
        "where $W_I$ is some intermediate $G$-representation.  This $M_I$ is a module over the algebra of invariant functions $R_I =  \\mathbb R[W_V^2 \\times W_E]^G$.\n",
        "\n",
        "We make the assumption that $R_I$ is a fininitely generated algebra with basis $f_1, \\ldots, f_n$ and that $M_I$ is finitely generated as an $R_I$-module with basis $\\mu_1, \\ldots, \\mu_N$ (for instance, this is guaranteed if $G$ is reductive).  We consider the following set of equivariant functions:\n",
        "$$\\{F^\\alpha \\colon \\alpha \\in A\\} = \\{\\mu_l\\} \\cup \\{f_k \\cdot \\mu_l\\}.$$\n",
        "Note that this set might not be linearly independent, there may be linear relations (syzygys) between the elements, leading to some potential redundancy in functions represented as linear combinations.\n",
        "\n",
        "The interpretation here is that we are generalizing the set of affine functions (sums of linear functions and constant functions) between vector spaces by including lowest order and next-to-lowest order generators.\n",
        "\n",
        "So we can now define the possible message functions.  These will take the form\n",
        "$$m_{ij} = \\sigma_I \\left(\\sum_{\\alpha \\in A} a_\\alpha f^\\alpha(h_i, h_j, a_{ij})  \\right)$$\n",
        "for some learnable coefficients $a_\\alpha$, and some pointwise activation function $\\sigma_I$.\n",
        "\n",
        "We can use these message functions to update the vertex and edge variables.  We will again construct sets of invariant functions\n",
        "\\begin{align}\n",
        "g^\\beta &\\in (\\mathbb R[W_V \\times W_I] \\otimes W_V^{\\mathrm{out}})^G \\\\\n",
        "k^\\gamma &\\in (\\mathbb R[W_E \\times W_I] \\otimes W_E^{\\mathrm{out}})^G\n",
        "\\end{align}\n",
        "in exactly the same way.  If we choose another activation function $\\sigma$ then the updated vertex and edge variables are given as follows:\n",
        "\\begin{align}\n",
        "h_i^{\\mathrm{out}} &= \\sigma \\left(b_\\beta g^\\beta\\left(h_i, \\sum_{v_j \\in N(v_i)} m_{ij} \\right) \\right) \\\\\n",
        "a_{ij}^{\\mathrm{out}} &= \\sigma \\left(c_\\gamma k^\\gamma\\left(a_{ij}, \\sum_{v_\\ell \\in N(v_i)} m_{i\\ell} + \\sum_{v_\\ell \\in N(v_j)} m_{j\\ell}  \\right) \\right)\n",
        "\\end{align}\n",
        "where again $b_\\beta, c_\\gamma$ are learnable weights, and where we write $N(v_i)$ for the neighborhood of vertex $v_i$ in the graph $\\Gamma$."
      ],
      "metadata": {
        "id": "umLhZLf2QfPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example 1:\n",
        "\n",
        "We can check that if $G$ is trivial then we recover a usual graph neural network architecture.  Indeed, in the trivial case, when we study functions $W_1 \\to W_2$, the generators in our model are given as follows.\n",
        "\n",
        "*   Generators of $W_2$ as an $\\mathbb R[W_1]$-module -- basis vectors $e^{(2)}_i$\n",
        "*   The product of generators of $W_2$ with algebra generators of $\\mathbb R[W_1]$ -- tensors of the form $(e^{(1)}_j)^* \\otimes e^{(2)}_i$.  In other words, matrix elements in $W_1^* \\otimes W_2$.\n",
        "\n",
        "Linear combinations of the first type of element generate constant functions $W_1 \\to W_2$, and linear combinations of the second type of element generate linear functions $W_1 \\to W_2$.  So altogether when we take arbitrary linear combinations in our model we are just considering the set of affine functions."
      ],
      "metadata": {
        "id": "bEeHFM9yiUmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example 2:\n",
        "\n",
        "Let's consider the example of Satorras, Hoogeboom and Welling.  So let $G = E(n)$, let $W_E$ be a trivial representation, and let $W_V = \\mathbb R^{n+1} \\times U$ where $U$ is again a trivial representation, and $E(n)$ acts on $\\mathbb R^{n+1} = \\mathbb R^n \\times \\mathbb R$ by affine transformations:\n",
        "$$(C,b) \\cdot (v, t) = (Cv + tb, t).$$\n",
        "We obtain the usual affine action on $\\mathbb R^n$ by restricting to the hyperplane $t=1$, and we will restrict attention to $E(n)$-equivariant functions that preserve this hyperplane.\n",
        "\n",
        "**One Spatial Input**\n",
        "\n",
        "Let us start by analyzing the equivariant functions of the form\n",
        "$$F \\colon \\mathbb R^{n+1} \\times U_1 \\to \\mathbb R^{n+1} \\times U_2$$\n",
        "where $U_1, U_2$ are trivial representations.  So according to our procedure we will need to compute algebra generators for $$A = \\mathbb R[\\mathbb R^{n+1} \\times U_1]^{E(n)}$$ and module generators for $$M = (\\mathbb R[\\mathbb R^{n+1} \\times U_1] \\otimes (\\mathbb R^{n+1} \\times U_2))^{E(n)}.$$  In each case we will restrict to those functions that preserve the $(n+1)^{\\text{st}}$ coordinate in $\\mathbb R^{n+1}$.\n",
        "\n",
        "In the first case $A \\cong \\mathbb R[\\mathbb R^{n+1}]^{E(n)} \\otimes \\mathbb R[U_1]$, and the only generators that preserve the final coordinate are constant in the first factor, so we have generators associated to basis vectors in $U_1$.\n",
        "\n",
        "In the latter case, $M$ is generated as a module by constant functions to $U_2$ together with the identity function $\\mathbb R^{n+1} \\to \\mathbb R^{n+1}$.  So, altogether, the set of zeroth and first order generating functions can be identified with $$\\{f^\\alpha\\} \\cong \\langle \\mathrm{id}\\rangle \\oplus U_2 \\oplus (U_1^* \\otimes \\langle\\mathrm{id}\\rangle) \\oplus (U_1^* \\otimes U_2).$$\n",
        "\n",
        "**Two Spatial Inputs**\n",
        "\n",
        "Finally, associated to the intermediate term we have a variant of this computation.  We need to compute algebra generators for $$A = \\mathbb R[(\\mathbb R^{n+1})^2 \\times U_1]^{E(n)}$$ and module generators for $$M = (\\mathbb R[(\\mathbb R^{n+1})^2 \\times U_1] \\otimes (\\mathbb R^{n+1} \\times U_2))^{E(n)},$$\n",
        "again preserving the hyperplane where the final coordinate in $\\mathbb R^{n+1}$ is equal to one.\n",
        "\n",
        "Let us use the notation $((x_1, t_1), (x_2, t_2)) \\in (\\mathbb R^{n+1})^2$. The algebra $A$ now has generators associated to basis vectors in $U_1$, but in addition we have a quadratic generator of the form $\\|x_1 - x_2\\|^2$.  The module $M$ still has module generators associated to constant functions to $U_2$, but rather than the identity there are now two additional generators associated to the projections $\\pi_1, \\pi_2$ onto the two factors of $(\\mathbb R^{n+1})^2$.  So now, altogether, the set of zeroth and first order generating functions can be identified with\n",
        "$$\\{f^\\alpha\\} \\cong \\langle \\pi_1, \\pi_2 \\rangle \\oplus U_2 \\oplus (U_1^* \\otimes \\langle \\pi_1, \\pi_2 \\rangle) \\oplus (\\langle \\|x_1 - x_2 \\|^2 \\rangle \\otimes \\langle \\pi_1, \\pi_2 \\rangle) \\oplus (U_1^* \\otimes U_2) \\oplus (U_1^* \\otimes \\langle \\|x_1 - x_2 \\|^2 \\rangle).$$\n",
        "In order to preserve the hyperplane we will need to restrict attention to those linear combinations $a_1 \\pi_1 + a_2 \\pi_2$ where $a_1 + a_2 = 1$.\n",
        "\n"
      ],
      "metadata": {
        "id": "OhTOk5LEgo0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation\n",
        "\n",
        "Let's go ahead and implement the equivariant graph convolution layer following this procedure.  Note that Satorras et al actually allow their message passing and vertex update terms to contain two layers: linear -> activation -> linear -> activation (where the second activation may be constant).  I will include this behaviour as an option if desired so that we can compare the results with one and two layers."
      ],
      "metadata": {
        "id": "RqdbXUj-UuYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EGCL(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_vertices: int,\n",
        "        adj_matrix: torch.Tensor,\n",
        "        vertex_inputs: int,\n",
        "        edge_inputs: int,\n",
        "        vertex_outputs: int,\n",
        "        edge_outputs: int,\n",
        "        inter_vars: int,\n",
        "        inter_invt_funs: list,\n",
        "        vertex_invt_funs: list,\n",
        "        edge_invt_funs: list,\n",
        "        inter_activation: callable,\n",
        "        vertex_activation: callable,\n",
        "        edge_activation: callable,\n",
        "        double_layer: bool = False,\n",
        "        inter_vars_2: int = None,\n",
        "        vertex_hidden: int = None,\n",
        "        edge_hidden: int = None,\n",
        "        inter_activation_2: callable = None,\n",
        "        vertex_activation_2: callable = None,\n",
        "        edge_activation_2: callable = None,\n",
        "        is_affine: bool = False,\n",
        "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Equivariant Graph Convolution Layer with optional double layer processing.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        num_vertices : int\n",
        "            Number of vertices in the graph\n",
        "        adj_matrix : torch.Tensor\n",
        "            Adjacency matrix of the graph (shape: [num_vertices, num_vertices])\n",
        "        vertex_inputs : int\n",
        "            Dimension of vertex input features\n",
        "        edge_inputs : int\n",
        "            Dimension of edge input features\n",
        "        vertex_outputs : int\n",
        "            Dimension of vertex output features\n",
        "        edge_outputs : int\n",
        "            Dimension of edge output features\n",
        "        inter_vars : int\n",
        "            Dimension of intermediate message features\n",
        "        inter_invt_funs : list\n",
        "            List of invariant functions for message passing\n",
        "        vertex_invt_funs : list\n",
        "            List of invariant functions for vertex updates\n",
        "        edge_invt_funs : list\n",
        "            List of invariant functions for edge updates\n",
        "        inter_activation : callable\n",
        "            Activation function for intermediate computations\n",
        "        vertex_activation : callable\n",
        "            Activation function for vertex updates\n",
        "        edge_activation : callable\n",
        "            Activation function for edge updates\n",
        "        double_layer : bool\n",
        "            Whether to use double layer processing (default: False)\n",
        "        inter_vars_2 : int\n",
        "            Dimension of second intermediate layer (default: same as inter_vars)\n",
        "        vertex_hidden : int\n",
        "            Dimension of hidden layer in vertex update (default: same as vertex_outputs)\n",
        "        edge_hidden : int\n",
        "            Dimension of hidden layer in edge update (default: same as edge_outputs)\n",
        "        inter_activation_2 : callable\n",
        "            Second activation for message passing (default: same as inter_activation)\n",
        "        vertex_activation_2 : callable\n",
        "            Second activation for vertex update (default: same as vertex_activation)\n",
        "        edge_activation_2 : callable\n",
        "            Second activation for edge update (default: same as edge_activation)\n",
        "        is_affine : bool\n",
        "            Whether to normalize outputs to preserve affine transformations\n",
        "        device : str\n",
        "            Device to run computations on\n",
        "        \"\"\"\n",
        "        super(EGCL, self).__init__()\n",
        "\n",
        "        # Validate inputs\n",
        "        if not isinstance(adj_matrix, torch.Tensor):\n",
        "            adj_matrix = torch.tensor(adj_matrix, dtype=torch.float32)\n",
        "        if adj_matrix.shape != (num_vertices, num_vertices):\n",
        "            raise ValueError(f\"adj_matrix shape {adj_matrix.shape} doesn't match num_vertices {num_vertices}\")\n",
        "\n",
        "        # Store basic parameters\n",
        "        self.device = device\n",
        "        self.num_vertices = num_vertices\n",
        "        self.adj_matrix = adj_matrix.to(device)\n",
        "        self.vertex_inputs = vertex_inputs\n",
        "        self.edge_inputs = edge_inputs\n",
        "        self.vertex_outputs = vertex_outputs\n",
        "        self.edge_outputs = edge_outputs\n",
        "        self.inter_vars = inter_vars\n",
        "        self.inter_invt_funs = inter_invt_funs\n",
        "        self.vertex_invt_funs = vertex_invt_funs\n",
        "        self.edge_invt_funs = edge_invt_funs\n",
        "        self.inter_activation = inter_activation\n",
        "        self.vertex_activation = vertex_activation\n",
        "        self.edge_activation = edge_activation\n",
        "        self.is_affine = is_affine\n",
        "\n",
        "        self.num_inter_invts = len(inter_invt_funs)\n",
        "        self.num_vertex_invts = len(vertex_invt_funs)\n",
        "        self.num_edge_invts = len(edge_invt_funs)\n",
        "\n",
        "        # Initialize first layer weights\n",
        "        n_in = 2*vertex_inputs + edge_inputs  # Total input dimension\n",
        "        n_out = inter_vars                    # Output dimension\n",
        "        std = torch.sqrt(torch.tensor(6.0/(n_in + n_out)))\n",
        "\n",
        "        self.inter_weights = nn.Parameter(\n",
        "            torch.randn(self.num_inter_invts, device=device) * std\n",
        "        )\n",
        "\n",
        "        # For vertex update weights\n",
        "        n_in = vertex_inputs + inter_vars     # Input vertex features + message features\n",
        "        n_out = vertex_outputs\n",
        "        std = torch.sqrt(torch.tensor(6.0/(n_in + n_out)))\n",
        "\n",
        "        self.vertex_weights = nn.Parameter(\n",
        "            torch.randn(self.num_vertex_invts, device=device) * std\n",
        "        )\n",
        "\n",
        "        # For edge update weights\n",
        "        n_in = edge_inputs + 2*inter_vars     # Edge features + sum of messages\n",
        "        n_out = edge_outputs\n",
        "        std = torch.sqrt(torch.tensor(6.0/(n_in + n_out)))\n",
        "\n",
        "        self.edge_weights = nn.Parameter(\n",
        "            torch.randn(self.num_edge_invts, device=device) * std\n",
        "        )\n",
        "\n",
        "        # Handle double layer parameters\n",
        "        self.double_layer = double_layer\n",
        "        if double_layer:\n",
        "            self.inter_vars_2 = inter_vars_2 if inter_vars_2 is not None else inter_vars\n",
        "            self.vertex_hidden = vertex_hidden if vertex_hidden is not None else vertex_outputs\n",
        "            self.edge_hidden = edge_hidden if edge_hidden is not None else edge_outputs\n",
        "            self.inter_activation_2 = inter_activation_2 if inter_activation_2 is not None else inter_activation\n",
        "            self.vertex_activation_2 = vertex_activation_2 if vertex_activation_2 is not None else vertex_activation\n",
        "            self.edge_activation_2 = edge_activation_2 if edge_activation_2 is not None else edge_activation\n",
        "\n",
        "            # Initialize second layer weights\n",
        "            n_in = inter_vars  # Total input dimension\n",
        "            n_out = inter_vars_2       # Output dimension\n",
        "            std = torch.sqrt(torch.tensor(6.0/(n_in + n_out)))\n",
        "\n",
        "            self.inter_weights_2 = nn.Parameter(\n",
        "                torch.randn(self.num_inter_vars_2, device=device) * std\n",
        "            )\n",
        "\n",
        "            # For vertex update weights\n",
        "            n_in = vertex_hidden   # Input vertex features + message features\n",
        "            n_out = vertex_outputs\n",
        "            std = torch.sqrt(torch.tensor(6.0/(n_in + n_out)))\n",
        "\n",
        "            self.vertex_weights_2 = nn.Parameter(\n",
        "                torch.randn(self.num_vertex_outputs, device=device) * std\n",
        "            )\n",
        "\n",
        "            # For edge update weights\n",
        "            n_in = edge_hidden    # Edge features + sum of messages\n",
        "            n_out = edge_outputs\n",
        "            std = torch.sqrt(torch.tensor(6.0/(n_in + n_out)))\n",
        "\n",
        "            self.edge_weights_2 = nn.Parameter(\n",
        "                torch.randn(self.num_edge_outputs, device=device) * std\n",
        "            )\n",
        "\n",
        "        # Pre-compute neighborhoods\n",
        "        self.neighborhoods = self._compute_neighborhoods()\n",
        "\n",
        "    def _compute_neighborhoods(self):\n",
        "        \"\"\"Pre-compute neighborhoods for each vertex.\"\"\"\n",
        "        neighborhoods = {}\n",
        "        for i in range(self.num_vertices):\n",
        "            neighborhoods[i] = torch.nonzero(self.adj_matrix[i], as_tuple=False).squeeze(1)\n",
        "        return neighborhoods\n",
        "\n",
        "    def _normalize_if_affine(self, tensor):\n",
        "        \"\"\"Normalize tensor if is_affine is True and tensor is non-zero.\"\"\"\n",
        "        if self.is_affine and torch.norm(tensor) > 1e-8:\n",
        "            return F.normalize(tensor, dim=-1)\n",
        "        return tensor\n",
        "\n",
        "    def intermediate_term(self, h_i, h_j, a_ij):\n",
        "        \"\"\"Compute message from vertex i to vertex j.\"\"\"\n",
        "        try:\n",
        "            # First layer\n",
        "            print(f\"Computing {len(self.inter_invt_funs)} invariant functions...\")\n",
        "            invt_outputs = torch.stack([\n",
        "                torch.as_tensor(f(h_i, h_j, a_ij), device=self.device)\n",
        "                for f in self.inter_invt_funs\n",
        "            ])\n",
        "            message = torch.matmul(self.inter_weights, invt_outputs)\n",
        "            message = self._normalize_if_affine(message)\n",
        "            message = self.inter_activation(message)\n",
        "\n",
        "            if not self.double_layer:\n",
        "                return message\n",
        "\n",
        "            # Second layer\n",
        "            message = torch.matmul(self.inter_weights_2, message)\n",
        "            message = self._normalize_if_affine(message)\n",
        "            return self.inter_activation_2(message)\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            raise RuntimeError(f\"Error in intermediate_term: {str(e)}\")\n",
        "\n",
        "    def vertex_update(self, h, m):\n",
        "        \"\"\"Update vertex features.\"\"\"\n",
        "        try:\n",
        "            # First layer\n",
        "            invt_outputs = torch.stack([\n",
        "                torch.as_tensor(f(h, m), device=self.device)\n",
        "                for f in self.vertex_invt_funs\n",
        "            ])\n",
        "            update = torch.matmul(self.vertex_weights, invt_outputs)\n",
        "            update = self._normalize_if_affine(update)\n",
        "            update = self.vertex_activation(update)\n",
        "\n",
        "            if not self.double_layer:\n",
        "                return update\n",
        "\n",
        "            # Second layer\n",
        "            update = torch.matmul(self.vertex_weights_2, update)\n",
        "            update = self._normalize_if_affine(update)\n",
        "            return self.vertex_activation_2(update)\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            raise RuntimeError(f\"Error in vertex_update: {str(e)}\")\n",
        "\n",
        "    def edge_update(self, a, m):\n",
        "        \"\"\"Update edge features.\"\"\"\n",
        "        try:\n",
        "            # First layer\n",
        "            invt_outputs = torch.stack([\n",
        "                torch.as_tensor(f(a, m), device=self.device)\n",
        "                for f in self.edge_invt_funs\n",
        "            ])\n",
        "            update = torch.matmul(self.edge_weights, invt_outputs)\n",
        "            update = self._normalize_if_affine(update)\n",
        "            update = self.edge_activation(update)\n",
        "\n",
        "            if not self.double_layer:\n",
        "                return update\n",
        "\n",
        "            # Second layer\n",
        "            update = torch.matmul(self.edge_weights_2, update)\n",
        "            update = self._normalize_if_affine(update)\n",
        "            return self.edge_activation_2(update)\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            raise RuntimeError(f\"Error in edge_update: {str(e)}\")\n",
        "\n",
        "    def forward(self, h_graph: torch.Tensor, a_graph: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass of the layer.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        h_graph : torch.Tensor\n",
        "            Vertex features (shape: [num_vertices, vertex_inputs])\n",
        "        a_graph : torch.Tensor\n",
        "            Edge features (shape: [num_vertices, num_vertices, edge_inputs])\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple(torch.Tensor, torch.Tensor)\n",
        "            Updated vertex and edge features\n",
        "        \"\"\"\n",
        "        if h_graph.shape != (self.num_vertices, self.vertex_inputs):\n",
        "            raise ValueError(f\"h_graph shape {h_graph.shape} doesn't match expected shape\"\n",
        "                           f\" ({self.num_vertices}, {self.vertex_inputs})\")\n",
        "        if a_graph.shape != (self.num_vertices, self.num_vertices, self.edge_inputs):\n",
        "            raise ValueError(f\"a_graph shape {a_graph.shape} doesn't match expected shape\"\n",
        "                           f\" ({self.num_vertices}, {self.num_vertices}, {self.edge_inputs})\")\n",
        "\n",
        "        # Move inputs to correct device\n",
        "        h_graph = h_graph.to(self.device)\n",
        "        a_graph = a_graph.to(self.device)\n",
        "\n",
        "        # Initialize output tensors\n",
        "        h_graph_out = torch.zeros(\n",
        "            (self.num_vertices, self.vertex_outputs),\n",
        "            device=self.device\n",
        "        )\n",
        "        a_graph_out = torch.zeros(\n",
        "            (self.num_vertices, self.num_vertices, self.edge_outputs),\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Compute messages for all edges\n",
        "        messages = torch.zeros(\n",
        "            (self.num_vertices, self.num_vertices, self.inter_vars),\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Get active edges\n",
        "        edge_indices = torch.nonzero(self.adj_matrix, as_tuple=True)\n",
        "        num_edges = len(edge_indices[0])\n",
        "        print(f\"Computing messages for {num_edges} active edges...\")\n",
        "\n",
        "        # Compute messages only for existing edges\n",
        "        for idx, (i, j) in enumerate(zip(*edge_indices)):\n",
        "            if idx % 5 == 0:  # Print progress every 5 edges\n",
        "                print(f\"Processing edge {idx+1}/{num_edges} ({i}->{j})\")\n",
        "            messages[i, j] = self.intermediate_term(h_graph[i], h_graph[j], a_graph[i, j])\n",
        "\n",
        "        print(\"Computing message sums...\")\n",
        "        message_sums = torch.sum(\n",
        "            messages * self.adj_matrix.unsqueeze(-1),\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        print(f\"Updating vertices...\")\n",
        "        for i in range(self.num_vertices):\n",
        "            print(f\"Vertex {i+1}/{self.num_vertices}\")\n",
        "            h_graph_out[i] = self.vertex_update(h_graph[i], message_sums[i])\n",
        "\n",
        "        print(f\"Updating edges...\")\n",
        "        edge_count = 0\n",
        "        for i, j in zip(*edge_indices):\n",
        "            if edge_count % 5 == 0:\n",
        "                print(f\"Edge {edge_count+1}/{num_edges}\")\n",
        "            a_graph_out[i, j] = self.edge_update(\n",
        "                a_graph[i, j],\n",
        "                message_sums[i] + message_sums[j]\n",
        "            )\n",
        "            edge_count += 1\n",
        "\n",
        "        print(\"Forward pass complete\")\n",
        "        return h_graph_out, a_graph_out"
      ],
      "metadata": {
        "id": "yTNOgXMePV4X"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by testing the layer with an example.  I'll let $G = E(3)$ and use the fundamental representation $\\mathbb R^4$ for the vertex space $W_V$ and intermediate space and the trivial representation $\\mathbb R$ for the edge space $W_E$.  I'll just list the invariant functions defined individually."
      ],
      "metadata": {
        "id": "VAxY7_T5n2uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_vertices = 5\n",
        "adj_matrix = torch.randint(0,2,(num_vertices, num_vertices))\n",
        "adj_matrix = (adj_matrix + adj_matrix.T) / 2\n",
        "\n",
        "def inter_invt_fun1(h_i, h_j, a_ij):\n",
        "    return h_i\n",
        "\n",
        "def inter_invt_fun2(h_i, h_j, a_ij):\n",
        "    return h_j\n",
        "\n",
        "def inter_invt_fun3(h_i, h_j, a_ij):\n",
        "    return a_ij * h_i\n",
        "\n",
        "def inter_invt_fun4(h_i, h_j, a_ij):\n",
        "    return a_ij * h_j\n",
        "\n",
        "def inter_invt_fun5(h_i, h_j, a_ij):\n",
        "    return h_i * torch.linalg.norm(h_i - h_j) ** 2\n",
        "\n",
        "def inter_invt_fun6(h_i, h_j, a_ij):\n",
        "    return h_j * torch.linalg.norm(h_i - h_j) ** 2\n",
        "\n",
        "def vertex_invt_fun1(h, m):\n",
        "    return h\n",
        "\n",
        "def vertex_invt_fun2(h, m):\n",
        "    return m\n",
        "\n",
        "def vertex_invt_fun3(h, m):\n",
        "    return h * torch.linalg.norm(h - m) ** 2\n",
        "\n",
        "def vertex_invt_fun4(h, m):\n",
        "    return m * torch.linalg.norm(h - m) ** 2\n",
        "\n",
        "\n",
        "def edge_invt_fun(a, m):\n",
        "    return a\n",
        "\n",
        "inter_invt_funs = [\n",
        "    inter_invt_fun1,\n",
        "    inter_invt_fun2,\n",
        "    inter_invt_fun3,\n",
        "    inter_invt_fun4,\n",
        "    inter_invt_fun5,\n",
        "    inter_invt_fun6\n",
        "]\n",
        "\n",
        "vertex_invt_funs = [\n",
        "    vertex_invt_fun1,\n",
        "    vertex_invt_fun2,\n",
        "    vertex_invt_fun3,\n",
        "    vertex_invt_fun4\n",
        "]\n",
        "\n",
        "edge_invt_funs = [edge_invt_fun]"
      ],
      "metadata": {
        "id": "N53llv26dhSr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = EGCL(\n",
        "    num_vertices=num_vertices,\n",
        "    adj_matrix=adj_matrix,\n",
        "    vertex_inputs=4,    # R^4 for vertex features\n",
        "    edge_inputs=1,      # R^1 for edge features (trivial rep)\n",
        "    vertex_outputs=4,   # R^4 output\n",
        "    edge_outputs=1,     # R^1 output\n",
        "    inter_vars=4,       # R^4 for intermediate representation\n",
        "    inter_invt_funs=inter_invt_funs,\n",
        "    vertex_invt_funs=vertex_invt_funs,\n",
        "    edge_invt_funs=edge_invt_funs,\n",
        "    inter_activation=torch.nn.ReLU(),\n",
        "    vertex_activation=torch.nn.ReLU(),\n",
        "    edge_activation=torch.nn.ReLU(),\n",
        "    is_affine=True     # We are using an affine group so should rescale to preserve a hyperplane\n",
        ")\n",
        "\n",
        "# Create some example input data\n",
        "h_graph = torch.randn(num_vertices, 4)  # Random vertex features in R^4\n",
        "a_graph = torch.randn(num_vertices, num_vertices, 1)  # Random edge features\n",
        "\n",
        "# Forward pass\n",
        "h_out, a_out = layer(h_graph, a_graph)\n",
        "\n",
        "print(f\"Input vertex features shape: {h_graph.shape}\")\n",
        "print(f\"Output vertex features shape: {h_out.shape}\")\n",
        "print(f\"Input edge features shape: {a_graph.shape}\")\n",
        "print(f\"Output edge features shape: {a_out.shape}\")\n",
        "\n",
        "# Print statistics before and after activation\n",
        "def print_activation_stats(tensor, name):\n",
        "    print(f\"\\n{name} statistics:\")\n",
        "    print(f\"Min: {tensor.min():.4f}\")\n",
        "    print(f\"Max: {tensor.max():.4f}\")\n",
        "    print(f\"Mean: {tensor.mean():.4f}\")\n",
        "    print(f\"Std: {tensor.std():.4f}\")\n",
        "\n",
        "# Get pre-activation values (you'd need to modify the layer to expose these)\n",
        "print_activation_stats(h_out, \"Vertex outputs\")\n",
        "print_activation_stats(a_out, \"Edge outputs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPpoPLEfvDYy",
        "outputId": "a0ce2487-393b-430b-9f3e-67fff970fedc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing messages for 15 active edges...\n",
            "Processing edge 1/15 (0->0)\n",
            "Computing 6 invariant functions...\n",
            "Computing 6 invariant functions...\n",
            "Computing 6 invariant functions...\n",
            "Computing 6 invariant functions...\n",
            "Computing 6 invariant functions...\n",
            "Processing edge 6/15 (1->3)\n",
            "Computing 6 invariant functions...\n",
            "Computing 6 invariant functions...\n",
            "Computing 6 invariant functions...\n",
            "Computing 6 invariant functions...\n",
            "Computing 6 invariant functions...\n",
            "Processing edge 11/15 (3->2)\n",
            "Computing 6 invariant functions...\n",
            "Computing 6 invariant functions...\n",
            "Computing 6 invariant functions...\n",
            "Computing 6 invariant functions...\n",
            "Computing 6 invariant functions...\n",
            "Computing message sums...\n",
            "Updating vertices...\n",
            "Vertex 1/5\n",
            "Vertex 2/5\n",
            "Vertex 3/5\n",
            "Vertex 4/5\n",
            "Vertex 5/5\n",
            "Updating edges...\n",
            "Edge 1/15\n",
            "Edge 6/15\n",
            "Edge 11/15\n",
            "Forward pass complete\n",
            "Input vertex features shape: torch.Size([5, 4])\n",
            "Output vertex features shape: torch.Size([5, 4])\n",
            "Input edge features shape: torch.Size([5, 5, 1])\n",
            "Output edge features shape: torch.Size([5, 5, 1])\n",
            "\n",
            "Vertex outputs statistics:\n",
            "Min: 0.0000\n",
            "Max: 0.9946\n",
            "Mean: 0.2856\n",
            "Std: 0.4102\n",
            "\n",
            "Edge outputs statistics:\n",
            "Min: 0.0000\n",
            "Max: 1.0000\n",
            "Mean: 0.3600\n",
            "Std: 0.4899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building $E(n)$ Equivariant Models\n",
        "\n",
        "The next step is to build a model.  I'll stick with the $E(n)$ example, and representations of the form $(\\mathbb R^{n+1})^a \\times \\mathbb R^b$ -- products of the fundamental and trivial representations.  I'll need a general method for constructing the sets of generating equivariant functions.\n",
        "\n",
        "Let's consider equivariant functions\n",
        "$$F \\colon (\\mathbb R^{n+1})^a \\times \\mathbb R^b \\to (\\mathbb R^{n+1})^c \\times \\mathbb R^d.$$\n",
        "The set of $\\mathrm O(n)$-equivariant functions we will wish to generate will consist of\n",
        "$$\\{x_{ij}, e_{kl}, \\langle x_p, x_q \\rangle x_{ij}, \\langle x_p, x_q \\rangle e_{kl}, e_r x_{ij}, e_r e_{kl}\\}$$\n",
        "where $x_{ij}, e_{kl}$ are the matrix element functions in the first and second factors respectively, and where $x_p, e_r$ are coordinate functions on the $p^\\text{th}$ factor of $(\\mathbb R^{n+1})^a$ and the $r^\\text{th}$ factor of $\\mathbb R^b$ respectively.  To be additionally translation equivariant we must restrict the inner product terms to those generated by $\\langle x_p, x_q \\rangle - \\langle x_p, x_{q'} \\rangle$, and the $x_{ij}$ terms to linear combinations with coefficients summing to one.\n"
      ],
      "metadata": {
        "id": "ziO65nhlVBrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Callable\n",
        "\n",
        "class EnRepresentation:\n",
        "    \"\"\"\n",
        "    Handles E(n) representations of the form (R^(n+1))^a Ã— R^b\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, num_vectors: int, num_scalars: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim: Dimension n of the ambient spacetime\n",
        "            num_vectors: Number of R^(n+1) factors\n",
        "            num_scalars: Dimension of factors\n",
        "        \"\"\"\n",
        "        self.dim = dim\n",
        "        self.num_vectors = num_vectors\n",
        "        self.num_scalars = num_scalars\n",
        "\n",
        "    def total_dim(self) -> int:\n",
        "        \"\"\"Total dimension of representation space.\"\"\"\n",
        "        return (self.dim + 1) * self.num_vectors + self.num_scalars\n",
        "\n",
        "    def split_vector_scalar(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Split tensor into vector and scalar parts.\"\"\"\n",
        "        vector_dim = (self.dim + 1) * self.num_vectors\n",
        "        return x[:vector_dim], x[vector_dim:]\n",
        "\n",
        "    def combine_vector_scalar(self, vectors: torch.Tensor, scalars: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Combine vector and scalar parts.\"\"\"\n",
        "        return torch.cat([vectors, scalars])\n",
        "\n",
        "def generate_En_message_invariants(\n",
        "    dim: int,\n",
        "    vertex_rep: EnRepresentation,  # Vertex rep\n",
        "    edge_rep: EnRepresentation,  # Edge rep\n",
        "    out_rep: EnRepresentation   # Output rep\n",
        ") -> List[Callable]:\n",
        "    \"\"\"\n",
        "    Generate E(n)-equivariant functions for message passing.\n",
        "    \"\"\"\n",
        "    invariants = []\n",
        "\n",
        "    def make_linear_vector_invariant(i: int, j: int) -> Callable:\n",
        "        \"\"\"Create linear invariant function for vector factor.\"\"\"\n",
        "        def f(h1: torch.Tensor, h2: torch.Tensor, e: torch.Tensor) -> torch.Tensor:\n",
        "            v1, s1 = vertex_rep.split_vector_scalar(h1)\n",
        "            v2, s2 = vertex_rep.split_vector_scalar(h2)\n",
        "            ve, se = edge_rep.split_vector_scalar(e)\n",
        "\n",
        "            out_v = torch.zeros((out_rep.num_vectors * (dim + 1)), device=h1.device)\n",
        "            out_s = torch.zeros(out_rep.num_scalars, device=h1.device)\n",
        "\n",
        "            if i < vertex_rep.num_vectors:\n",
        "                out_v[j*(dim + 1):(j+1)*(dim + 1)] = v1[i*(dim + 1):(i+1)*(dim + 1)]\n",
        "            elif i < vertex_rep.num_vectors * 2:\n",
        "                i2 = i - vertex_rep.num_vectors\n",
        "                out_v[j*(dim + 1):(j+1)*(dim + 1)] = v2[i2*(dim + 1):(i2+1)*(dim + 1)]\n",
        "\n",
        "            return out_rep.combine_vector_scalar(out_v, out_s)\n",
        "        return f\n",
        "\n",
        "    def make_linear_scalar_invariant(k: int, l: int) -> Callable:\n",
        "        \"\"\"Create linear invariant function for scalar factor.\"\"\"\n",
        "        def f(h1: torch.Tensor, h2: torch.Tensor, e: torch.Tensor) -> torch.Tensor:\n",
        "          v1, s1 = vertex_rep.split_vector_scalar(h1)\n",
        "          v2, s2 = vertex_rep.split_vector_scalar(h2)\n",
        "          ve, se = edge_rep.split_vector_scalar(e)\n",
        "\n",
        "          out_v = torch.zeros((out_rep.num_vectors * (dim + 1)), device=h1.device)\n",
        "          out_s = torch.zeros(out_rep.num_scalars, device=h1.device)\n",
        "\n",
        "          if k < vertex_rep.num_scalars:\n",
        "            out_s[l] = s1[k]\n",
        "          elif k < vertex_rep.num_scalars * 2:\n",
        "            k2 = k - vertex_rep.num_scalars\n",
        "            out_s[l] = s2[k2]\n",
        "\n",
        "          return out_rep.combine_vector_scalar(out_v, out_s)\n",
        "        return f\n",
        "\n",
        "    def make_quadratic_vector_invariant(i: int, j: int, r: int) -> Callable:\n",
        "        \"\"\"Create quadratic invariant function x_ij e_r.\"\"\"\n",
        "        def f(h1: torch.Tensor, h2: torch.Tensor, e: torch.Tensor) -> torch.Tensor:\n",
        "            v1, s1 = vertex_rep.split_vector_scalar(h1)\n",
        "            v2, s2 = vertex_rep.split_vector_scalar(h2)\n",
        "            ve, se = edge_rep.split_vector_scalar(e)\n",
        "\n",
        "            out_v = torch.zeros((out_rep.num_vectors * (dim + 1)), device=h1.device)\n",
        "            out_s = torch.zeros(out_rep.num_scalars, device=h1.device)\n",
        "\n",
        "            g = make_linear_vector_invariant(i,j)\n",
        "            lin_v, lin_s = out_rep.split_vector_scalar(g(h1, h2, e))\n",
        "            if r < vertex_rep.num_scalars:\n",
        "                out_v = lin_v * s1[r]\n",
        "            elif r < vertex_rep.num_scalars * 2:\n",
        "                r2 = r - vertex_rep.num_scalars\n",
        "                out_v = lin_v * s2[r2]\n",
        "\n",
        "            return out_rep.combine_vector_scalar(out_v, out_s)\n",
        "        return f\n",
        "\n",
        "    def make_quadratic_scalar_invariant(k: int, l: int, r: int) -> Callable:\n",
        "        \"\"\"Create quadratic invariant function e_kl e_r.\"\"\"\n",
        "        def f(h1: torch.Tensor, h2: torch.Tensor, e: torch.Tensor) -> torch.Tensor:\n",
        "            v1, s1 = vertex_rep.split_vector_scalar(h1)\n",
        "            v2, s2 = vertex_rep.split_vector_scalar(h2)\n",
        "            ve, se = edge_rep.split_vector_scalar(e)\n",
        "\n",
        "            out_v = torch.zeros((out_rep.num_vectors * (dim + 1)), device=h1.device)\n",
        "            out_s = torch.zeros(out_rep.num_scalars, device=h1.device)\n",
        "\n",
        "            g = make_linear_scalar_invariant(k,l)\n",
        "            lin_v, lin_s = out_rep.split_vector_scalar(g(h1, h2, e))\n",
        "            if r < vertex_rep.num_scalars:\n",
        "                out_s = lin_s * s1[r]\n",
        "            elif r < vertex_rep.num_scalars * 2:\n",
        "                r2 = r - vertex_rep.num_scalars\n",
        "                out_s = lin_s * s2[r2]\n",
        "\n",
        "            return out_rep.combine_vector_scalar(out_v, out_s)\n",
        "        return f\n",
        "\n",
        "    def make_cubic_vector_invariant(i: int, j: int, p: int, q: int) -> Callable:\n",
        "        \"\"\"Create cubic invariant function <x_p, x_q> x_ij.\"\"\"\n",
        "        def f(h1: torch.Tensor, h2: torch.Tensor, e: torch.Tensor) -> torch.Tensor:\n",
        "          v1, s1 = vertex_rep.split_vector_scalar(h1)\n",
        "          v2, s2 = vertex_rep.split_vector_scalar(h2)\n",
        "          ve, se = edge_rep.split_vector_scalar(e)\n",
        "\n",
        "          out_v = torch.zeros((out_rep.num_vectors * (dim + 1)), device=h1.device)\n",
        "          out_s = torch.zeros(out_rep.num_scalars, device=h1.device)\n",
        "\n",
        "          if p < vertex_rep.num_vectors:\n",
        "            w1 = v1[p*(dim + 1):(p+1)*(dim + 1)]\n",
        "          elif p < vertex_rep.num_vectors * 2:\n",
        "            p2 = p - vertex_rep.num_vectors\n",
        "            w1 = v2[p2*(dim + 1):(p2+1)*(dim + 1)]\n",
        "\n",
        "          if q < vertex_rep.num_vectors:\n",
        "            w2 = v1[q*(dim + 1):(q+1)*(dim + 1)]\n",
        "          elif q < vertex_rep.num_vectors * 2:\n",
        "            q2 = q - vertex_rep.num_vectors\n",
        "            w2 = v2[q2*(dim + 1):(q2+1)*(dim + 1)]\n",
        "\n",
        "          g = make_linear_vector_invariant(i,j)\n",
        "          lin_v, lin_s = out_rep.split_vector_scalar(g(h1, h2, e))\n",
        "          out_v = lin_v * torch.dot(w1, w2)\n",
        "\n",
        "          return out_rep.combine_vector_scalar(out_v, out_s)\n",
        "        return f\n",
        "\n",
        "    def make_cubic_scalar_invariant(k: int, l: int, p: int, q: int) -> Callable:\n",
        "        \"\"\"Create cubic invariant function <x_p, x_q> e_kl.\"\"\"\n",
        "        def f(h1: torch.Tensor, h2: torch.Tensor, e: torch.Tensor) -> torch.Tensor:\n",
        "          v1, s1 = vertex_rep.split_vector_scalar(h1)\n",
        "          v2, s2 = vertex_rep.split_vector_scalar(h2)\n",
        "          ve, se = edge_rep.split_vector_scalar(e)\n",
        "\n",
        "          out_v = torch.zeros((out_rep.num_vectors * (dim + 1)), device=h1.device)\n",
        "          out_s = torch.zeros(out_rep.num_scalars, device=h1.device)\n",
        "\n",
        "          if p < vertex_rep.num_vectors:\n",
        "            w1 = v1[p*(dim + 1):(p+1)*(dim + 1)]\n",
        "          elif p < vertex_rep.num_vectors * 2:\n",
        "            p2 = p - vertex_rep.num_vectors\n",
        "            w1 = v2[p2*(dim + 1):(p2+1)*(dim + 1)]\n",
        "\n",
        "          if q < vertex_rep.num_vectors:\n",
        "            w2 = v1[q*(dim + 1):(q+1)*(dim + 1)]\n",
        "          elif q < vertex_rep.num_vectors * 2:\n",
        "            q2 = q - vertex_rep.num_vectors\n",
        "            w2 = v2[q2*(dim + 1):(q2+1)*(dim + 1)]\n",
        "\n",
        "          g = make_linear_scalar_invariant(k,l)\n",
        "          lin_v, lin_s = out_rep.split_vector_scalar(g(h1, h2, e))\n",
        "          out_v = lin_v * torch.dot(w1, w2)\n",
        "\n",
        "          return out_rep.combine_vector_scalar(out_v, out_s)\n",
        "        return f\n",
        "\n",
        "\n",
        "    # Add linear invariants\n",
        "    for i in range(vertex_rep.num_vectors *2):\n",
        "        for j in range(out_rep.num_vectors):\n",
        "            invariants.append(make_linear_vector_invariant(i, j))\n",
        "\n",
        "    for k in range(vertex_rep.num_scalars *2):\n",
        "        for l in range(out_rep.num_scalars):\n",
        "            invariants.append(make_linear_scalar_invariant(k, l))\n",
        "\n",
        "    # Add quadratic invariants\n",
        "    for i in range(vertex_rep.num_vectors *2):\n",
        "        for j in range(out_rep.num_vectors):\n",
        "            for r in range(vertex_rep.num_scalars * 2):\n",
        "                invariants.append(make_quadratic_vector_invariant(i, j, r))\n",
        "\n",
        "    for k in range(vertex_rep.num_scalars *2):\n",
        "        for l in range(out_rep.num_scalars):\n",
        "            for r in range(vertex_rep.num_scalars * 2):\n",
        "              invariants.append(make_quadratic_scalar_invariant(k, l, r))\n",
        "\n",
        "    # Add cubic invariants.  We need to generate differences like <x_p, x_q - x_{q+1}>x_ij\n",
        "\n",
        "    def make_difference(f1, f2):\n",
        "                      def diff(h1, h2, e):\n",
        "                          return f1(h1, h2, e) - f2(h1, h2, e)\n",
        "                      return diff\n",
        "\n",
        "    for i in range(vertex_rep.num_vectors *2):\n",
        "        for j in range(out_rep.num_vectors):\n",
        "            for p in range(vertex_rep.num_vectors *2):\n",
        "                for q in range(vertex_rep.num_vectors):\n",
        "                  next_q = (q + 1) % vertex_rep.num_vectors  # Cyclic index\n",
        "                  f_current = make_cubic_vector_invariant(i, j, p, q)\n",
        "                  f_next = make_cubic_vector_invariant(i, j, p, next_q)\n",
        "\n",
        "                  invariants.append(make_difference(f_current, f_next))\n",
        "\n",
        "                base_q = vertex_rep.num_vectors\n",
        "                for q_offset in range(vertex_rep.num_vectors):\n",
        "                   q = base_q + q_offset\n",
        "                   next_q = base_q + ((q_offset + 1) % vertex_rep.num_vectors)\n",
        "                   g_current = make_cubic_vector_invariant(i, j, p, q)\n",
        "                   g_next = make_cubic_vector_invariant(i, j, p, next_q)\n",
        "\n",
        "                   invariants.append(make_difference(g_current, g_next))\n",
        "\n",
        "    for k in range(vertex_rep.num_scalars *2):\n",
        "        for l in range(out_rep.num_scalars):\n",
        "          for p in range(vertex_rep.num_vectors *2):\n",
        "                for q in range(vertex_rep.num_vectors):\n",
        "                  next_q = (q + 1) % vertex_rep.num_vectors  # Cyclic index\n",
        "                  f_current = make_cubic_scalar_invariant(k, l, p, q)\n",
        "                  f_next = make_cubic_scalar_invariant(k, l, p, next_q)\n",
        "\n",
        "                  invariants.append(make_difference(f_current, f_next))\n",
        "\n",
        "                base_q = vertex_rep.num_vectors\n",
        "                for q_offset in range(vertex_rep.num_vectors):\n",
        "                   q = base_q + q_offset\n",
        "                   next_q = base_q + ((q_offset + 1) % vertex_rep.num_vectors)\n",
        "                   g_current = make_cubic_scalar_invariant(k, l, p, q)\n",
        "                   g_next = make_cubic_scalar_invariant(k, l, p, next_q)\n",
        "\n",
        "                   invariants.append(make_difference(g_current, g_next))\n",
        "\n",
        "\n",
        "    return invariants\n",
        "\n",
        "def generate_En_vertex_edge_invariants(\n",
        "    dim: int,\n",
        "    input_rep: EnRepresentation,  # Vertex or edge rep\n",
        "    internal_rep: EnRepresentation,  # Internal variable rep\n",
        "    out_rep: EnRepresentation   # Output rep\n",
        ") -> List[Callable]:\n",
        "    \"\"\"\n",
        "    Generate E(n)-equivariant functions for message passing.\n",
        "    \"\"\"\n",
        "    invariants = []\n",
        "\n",
        "    def make_linear_vector_invariant(i: int, j: int) -> Callable:\n",
        "        \"\"\"Create linear invariant function for vector factor.\"\"\"\n",
        "        def f(h: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "            v, s = input_rep.split_vector_scalar(h)\n",
        "            vi, si = internal_rep.split_vector_scalar(m)\n",
        "\n",
        "            out_v = torch.zeros((out_rep.num_vectors * (dim + 1)), device=h.device)\n",
        "            out_s = torch.zeros(out_rep.num_scalars, device=h.device)\n",
        "\n",
        "            if i < input_rep.num_vectors:\n",
        "                out_v[j*(dim + 1):(j+1)*(dim + 1)] = v[i*(dim + 1):(i+1)*(dim + 1)]\n",
        "            elif i < input_rep.num_vectors + internal_rep.num_vectors:\n",
        "                i2 = i - input_rep.num_vectors\n",
        "                out_v[j*(dim + 1):(j+1)*(dim + 1)] = vi[i2*(dim + 1):(i2+1)*(dim + 1)]\n",
        "\n",
        "            return out_rep.combine_vector_scalar(out_v, out_s)\n",
        "        return f\n",
        "\n",
        "    def make_linear_scalar_invariant(k: int, l: int) -> Callable:\n",
        "        \"\"\"Create linear invariant function for scalar factor.\"\"\"\n",
        "        def f(h: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "          v, s = input_rep.split_vector_scalar(h)\n",
        "          vi, si = internal_rep.split_vector_scalar(m)\n",
        "\n",
        "          out_v = torch.zeros((out_rep.num_vectors * (dim + 1)), device=h.device)\n",
        "          out_s = torch.zeros(out_rep.num_scalars, device=h.device)\n",
        "\n",
        "          if k < input_rep.num_scalars:\n",
        "            out_s[l] = s[k]\n",
        "          elif k < input_rep.num_scalars + internal_rep.num_scalars:\n",
        "            k2 = k - input_rep.num_scalars\n",
        "            out_s[l] = si[k2]\n",
        "\n",
        "          return out_rep.combine_vector_scalar(out_v, out_s)\n",
        "        return f\n",
        "\n",
        "    def make_quadratic_vector_invariant(i: int, j: int, r: int) -> Callable:\n",
        "        \"\"\"Create quadratic invariant function x_ij e_r.\"\"\"\n",
        "        def f(h: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "            v, s = input_rep.split_vector_scalar(h)\n",
        "            vi, si = internal_rep.split_vector_scalar(m)\n",
        "\n",
        "            out_v = torch.zeros((out_rep.num_vectors * (dim + 1)), device=h.device)\n",
        "            out_s = torch.zeros(out_rep.num_scalars, device=h.device)\n",
        "\n",
        "            g = make_linear_vector_invariant(i,j)\n",
        "            lin_v, lin_s = out_rep.split_vector_scalar(g(h, m))\n",
        "            if r < input_rep.num_scalars:\n",
        "                out_v = lin_v * s[r]\n",
        "            elif r < input_rep.num_scalars + internal_rep.num_scalars:\n",
        "                r2 = r - input_rep.num_scalars\n",
        "                out_v = lin_v * si[r2]\n",
        "\n",
        "            return out_rep.combine_vector_scalar(out_v, out_s)\n",
        "        return f\n",
        "\n",
        "    def make_quadratic_scalar_invariant(k: int, l: int, r: int) -> Callable:\n",
        "        \"\"\"Create quadratic invariant function e_kl e_r.\"\"\"\n",
        "        def f(h: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "          v, s = input_rep.split_vector_scalar(h)\n",
        "          vi, si = internal_rep.split_vector_scalar(m)\n",
        "\n",
        "          out_v = torch.zeros((out_rep.num_vectors * (dim + 1)), device=h.device)\n",
        "          out_s = torch.zeros(out_rep.num_scalars, device=h.device)\n",
        "\n",
        "          g = make_linear_scalar_invariant(k,l)\n",
        "          lin_v, lin_s = out_rep.split_vector_scalar(g(h, m))\n",
        "          if r < input_rep.num_scalars:\n",
        "              out_s = lin_s * s[r]\n",
        "          elif r < input_rep.num_scalars + internal_rep.num_scalars:\n",
        "              r2 = r - input_rep.num_scalars\n",
        "              out_s = lin_s * si[r2]\n",
        "\n",
        "          return out_rep.combine_vector_scalar(out_v, out_s)\n",
        "        return f\n",
        "\n",
        "    def make_cubic_vector_invariant(i: int, j: int, p: int, q: int) -> Callable:\n",
        "        \"\"\"Create cubic invariant function <x_p, x_q> x_ij.\"\"\"\n",
        "        def f(h: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "          v, s = input_rep.split_vector_scalar(h)\n",
        "          vi, si = internal_rep.split_vector_scalar(m)\n",
        "\n",
        "          out_v = torch.zeros((out_rep.num_vectors * (dim + 1)), device=h.device)\n",
        "          out_s = torch.zeros(out_rep.num_scalars, device=h.device)\n",
        "\n",
        "          if p < input_rep.num_vectors:\n",
        "            w1 = v[p*(dim + 1):(p+1)*(dim + 1)]\n",
        "          elif p < input_rep.num_vectors + internal_rep.num_vectors:\n",
        "            p2 = p - input_rep.num_vectors\n",
        "            w1 = vi[p2*(dim + 1):(p2+1)*(dim + 1)]\n",
        "\n",
        "          if q < input_rep.num_vectors:\n",
        "            w2 = v[q*(dim + 1):(q+1)*(dim + 1)]\n",
        "          elif q < input_rep.num_vectors + internal_rep.num_vectors:\n",
        "            q2 = q - input_rep.num_vectors\n",
        "            w2 = vi[q2*(dim + 1):(q2+1)*(dim + 1)]\n",
        "\n",
        "          g = make_linear_vector_invariant(i,j)\n",
        "          lin_v, lin_s = out_rep.split_vector_scalar(g(h, m))\n",
        "          out_v = lin_v * torch.dot(w1, w2)\n",
        "\n",
        "          return out_rep.combine_vector_scalar(out_v, out_s)\n",
        "        return f\n",
        "\n",
        "    def make_cubic_scalar_invariant(k: int, l: int, p: int, q: int) -> Callable:\n",
        "        \"\"\"Create cubic invariant function <x_p, x_q> e_kl.\"\"\"\n",
        "        def f(h: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "          v, s = input_rep.split_vector_scalar(h)\n",
        "          vi, si = internal_rep.split_vector_scalar(m)\n",
        "\n",
        "          out_v = torch.zeros((out_rep.num_vectors * (dim + 1)), device=h.device)\n",
        "          out_s = torch.zeros(out_rep.num_scalars, device=h.device)\n",
        "\n",
        "          if p < input_rep.num_vectors:\n",
        "            w1 = v[p*(dim + 1):(p+1)*(dim + 1)]\n",
        "          elif p < input_rep.num_vectors + internal_rep.num_vectors:\n",
        "            p2 = p - input_rep.num_vectors\n",
        "            w1 = vi[p2*(dim + 1):(p2+1)*(dim + 1)]\n",
        "\n",
        "          if q < input_rep.num_vectors:\n",
        "            w2 = v[q*(dim + 1):(q+1)*(dim + 1)]\n",
        "          elif q < input_rep.num_vectors + internal_rep.num_vectors:\n",
        "            q2 = q - input_rep.num_vectors\n",
        "            w2 = vi[q2*(dim + 1):(q2+1)*(dim + 1)]\n",
        "\n",
        "          g = make_linear_scalar_invariant(k,l)\n",
        "          lin_v, lin_s = out_rep.split_vector_scalar(g(h, m))\n",
        "          out_s = lin_s * torch.dot(w1, w2)\n",
        "\n",
        "          return out_rep.combine_vector_scalar(out_v, out_s)\n",
        "        return f\n",
        "\n",
        "\n",
        "    # Add linear invariants\n",
        "    for i in range(input_rep.num_vectors + internal_rep.num_vectors):\n",
        "        for j in range(out_rep.num_vectors):\n",
        "            invariants.append(make_linear_vector_invariant(i, j))\n",
        "\n",
        "    for k in range(input_rep.num_scalars + internal_rep.num_scalars):\n",
        "        for l in range(out_rep.num_scalars):\n",
        "            invariants.append(make_linear_scalar_invariant(k, l))\n",
        "\n",
        "    # Add quadratic invariants\n",
        "    for i in range(input_rep.num_vectors + internal_rep.num_vectors):\n",
        "        for j in range(out_rep.num_vectors):\n",
        "            for r in range(input_rep.num_scalars + internal_rep.num_scalars):\n",
        "                invariants.append(make_quadratic_vector_invariant(i, j, r))\n",
        "\n",
        "    for k in range(input_rep.num_scalars + internal_rep.num_scalars):\n",
        "        for l in range(out_rep.num_scalars):\n",
        "            for r in range(input_rep.num_scalars + internal_rep.num_scalars):\n",
        "              invariants.append(make_quadratic_scalar_invariant(k, l, r))\n",
        "\n",
        "    # Add cubic invariants.  We need to generate differences like <x_p, x_q - x_{q+1}>x_ij\n",
        "\n",
        "    def make_difference(f1, f2):\n",
        "                      def diff(h, m):\n",
        "                          return f1(h, m) - f2(h, m)\n",
        "                      return diff\n",
        "\n",
        "    for i in range(input_rep.num_vectors + internal_rep.num_vectors):\n",
        "        for j in range(out_rep.num_vectors):\n",
        "            for p in range(input_rep.num_vectors + internal_rep.num_vectors):\n",
        "                for q in range(input_rep.num_vectors):\n",
        "                  next_q = (q + 1) % input_rep.num_vectors  # Cyclic index\n",
        "                  f_current = make_cubic_vector_invariant(i, j, p, q)\n",
        "                  f_next = make_cubic_vector_invariant(i, j, p, next_q)\n",
        "\n",
        "                  invariants.append(make_difference(f_current, f_next))\n",
        "\n",
        "                base_q = input_rep.num_vectors\n",
        "                for q_offset in range(internal_rep.num_vectors):\n",
        "                   q = base_q + q_offset\n",
        "                   next_q = base_q + ((q_offset + 1) % internal_rep.num_vectors)\n",
        "                   g_current = make_cubic_vector_invariant(i, j, p, q)\n",
        "                   g_next = make_cubic_vector_invariant(i, j, p, next_q)\n",
        "\n",
        "                   invariants.append(make_difference(g_current, g_next))\n",
        "\n",
        "    for k in range(input_rep.num_scalars + internal_rep.num_scalars):\n",
        "        for l in range(out_rep.num_scalars):\n",
        "          for p in range(input_rep.num_vectors + internal_rep.num_vectors):\n",
        "                for q in range(input_rep.num_vectors):\n",
        "                  next_q = (q + 1) % input_rep.num_vectors  # Cyclic index\n",
        "                  f_current = make_cubic_scalar_invariant(k, l, p, q)\n",
        "                  f_next = make_cubic_scalar_invariant(k, l, p, next_q)\n",
        "\n",
        "                  invariants.append(make_difference(f_current, f_next))\n",
        "\n",
        "                base_q = input_rep.num_vectors\n",
        "                for q_offset in range(internal_rep.num_vectors):\n",
        "                   q = base_q + q_offset\n",
        "                   next_q = base_q + ((q_offset + 1) % internal_rep.num_vectors)\n",
        "                   g_current = make_cubic_scalar_invariant(k, l, p, q)\n",
        "                   g_next = make_cubic_scalar_invariant(k, l, p, next_q)\n",
        "\n",
        "                   invariants.append(make_difference(g_current, g_next))\n",
        "\n",
        "\n",
        "    return invariants"
      ],
      "metadata": {
        "id": "eRRfLPgOkwjz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we are able to generate invariant functions we can build a model involving several $E(n)$-equivariant layers.  Let's build an $E(3)$-equivariant model with three hidden layers.  The input and output representations will be defined with $W_V = (\\mathbb{R}^4) \\times \\mathbb{R}^{n_f}$, and $W_E = \\mathbb{R}$.  Let's specifically set $n_f = 2$.\n",
        "\n",
        "I'll keep the edge representation trivial in intermediate layers, but allow the vertex representation to vary.  Initially let's say we have the following intermediate layer vertex representations:\n",
        "\\begin{align}\n",
        "W_V^{(2)} &= (\\mathbb R^4)^2 \\times \\mathbb R^2 \\\\\n",
        "W_V^{(3)} &= (\\mathbb R^4)^3 \\times \\mathbb R^3 \\\\\n",
        "W_V^{(4)} &= (\\mathbb R^4)^2 \\times \\mathbb R^2.\n",
        "\\end{align}\n",
        "In each layer I'll set the internal message representation equal to the vertex representation.  I won't use the double layer option for now, and I'll use ReLU activations throughout.\n"
      ],
      "metadata": {
        "id": "FCk-L9QAIVIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dim = 3\n",
        "\n",
        "num_vertices = 4\n",
        "adj_matrix = torch.randint(0,2,(num_vertices, num_vertices))\n",
        "adj_matrix = (adj_matrix + adj_matrix.T) / 2\n",
        "\n",
        "# Define representations\n",
        "input_vertex_rep = EnRepresentation(dim=3, num_vectors=1, num_scalars=2)\n",
        "edge_rep = EnRepresentation(dim=3, num_vectors=0, num_scalars=1)\n",
        "message_rep_1 = input_vertex_rep\n",
        "hidden_vertex_rep_1 = EnRepresentation(dim=3, num_vectors=2, num_scalars=2)\n",
        "hidden_message_rep_1 = hidden_vertex_rep_1\n",
        "hidden_vertex_rep_2 = EnRepresentation(dim=3, num_vectors=3, num_scalars=3)\n",
        "hidden_message_rep_2 = hidden_vertex_rep_2\n",
        "hidden_vertex_rep_3 = EnRepresentation(dim=3, num_vectors=2, num_scalars=2)\n",
        "hidden_message_rep_3 = hidden_vertex_rep_3\n",
        "output_vertex_rep = EnRepresentation(dim=3, num_vectors=1, num_scalars=2)\n",
        "\n",
        "# Layer 1\n",
        "message_invariants_1 = generate_En_message_invariants(\n",
        "    dim = dim,\n",
        "    vertex_rep = input_vertex_rep,\n",
        "    edge_rep = edge_rep,\n",
        "    out_rep = message_rep_1\n",
        ")\n",
        "vertex_invariants_1 = generate_En_vertex_edge_invariants(\n",
        "    dim = dim,\n",
        "    input_rep = input_vertex_rep,\n",
        "    internal_rep = message_rep_1,\n",
        "    out_rep = hidden_vertex_rep_1\n",
        ")\n",
        "edge_invariants_1 = generate_En_vertex_edge_invariants(\n",
        "    dim = dim,\n",
        "    input_rep = edge_rep,\n",
        "    internal_rep = message_rep_1,\n",
        "    out_rep = edge_rep\n",
        ")\n",
        "\n",
        "layer1 = EGCL(\n",
        "    num_vertices = num_vertices,\n",
        "    adj_matrix = adj_matrix,\n",
        "    vertex_inputs=input_vertex_rep.total_dim(),\n",
        "    edge_inputs=edge_rep.total_dim(),\n",
        "    vertex_outputs=hidden_vertex_rep_1.total_dim(),\n",
        "    edge_outputs=edge_rep.total_dim(),\n",
        "    inter_vars=message_rep_1.total_dim(),\n",
        "    inter_invt_funs=message_invariants_1,\n",
        "    vertex_invt_funs=vertex_invariants_1,\n",
        "    edge_invt_funs=edge_invariants_1,\n",
        "    inter_activation=torch.nn.ReLU(),\n",
        "    vertex_activation=torch.nn.ReLU(),\n",
        "    edge_activation=torch.nn.ReLU(),\n",
        "    is_affine=True\n",
        ")\n",
        "\n",
        "# Layer 2\n",
        "message_invariants_2 = generate_En_message_invariants(\n",
        "    dim = dim,\n",
        "    vertex_rep = hidden_vertex_rep_1,\n",
        "    edge_rep = edge_rep,\n",
        "    out_rep = hidden_message_rep_1\n",
        ")\n",
        "vertex_invariants_2 = generate_En_vertex_edge_invariants(\n",
        "    dim = dim,\n",
        "    input_rep = hidden_vertex_rep_1,\n",
        "    internal_rep = hidden_message_rep_1,\n",
        "    out_rep = hidden_vertex_rep_2\n",
        ")\n",
        "edge_invariants_2 = generate_En_vertex_edge_invariants(\n",
        "    dim = dim,\n",
        "    input_rep = edge_rep,\n",
        "    internal_rep = hidden_message_rep_1,\n",
        "    out_rep = edge_rep\n",
        ")\n",
        "\n",
        "layer2 = EGCL(\n",
        "    num_vertices = num_vertices,\n",
        "    adj_matrix = adj_matrix,\n",
        "    vertex_inputs=hidden_vertex_rep_1.total_dim(),\n",
        "    edge_inputs=edge_rep.total_dim(),\n",
        "    vertex_outputs=hidden_vertex_rep_2.total_dim(),\n",
        "    edge_outputs=edge_rep.total_dim(),\n",
        "    inter_vars=hidden_message_rep_1.total_dim(),\n",
        "    inter_invt_funs=message_invariants_2,\n",
        "    vertex_invt_funs=vertex_invariants_2,\n",
        "    edge_invt_funs=edge_invariants_2,\n",
        "    inter_activation=torch.nn.ReLU(),\n",
        "    vertex_activation=torch.nn.ReLU(),\n",
        "    edge_activation=torch.nn.ReLU(),\n",
        "    is_affine=True\n",
        ")\n",
        "\n",
        "# Layer 3\n",
        "message_invariants_3 = generate_En_message_invariants(\n",
        "    dim = dim,\n",
        "    vertex_rep = hidden_vertex_rep_2,\n",
        "    edge_rep = edge_rep,\n",
        "    out_rep = hidden_message_rep_2\n",
        ")\n",
        "vertex_invariants_3 = generate_En_vertex_edge_invariants(\n",
        "    dim = dim,\n",
        "    input_rep = hidden_vertex_rep_2,\n",
        "    internal_rep = hidden_message_rep_2,\n",
        "    out_rep = hidden_vertex_rep_3\n",
        ")\n",
        "edge_invariants_3 = generate_En_vertex_edge_invariants(\n",
        "    dim = dim,\n",
        "    input_rep = edge_rep,\n",
        "    internal_rep = hidden_message_rep_2,\n",
        "    out_rep = edge_rep\n",
        ")\n",
        "\n",
        "layer3 = EGCL(\n",
        "    num_vertices = num_vertices,\n",
        "    adj_matrix = adj_matrix,\n",
        "    vertex_inputs=hidden_vertex_rep_2.total_dim(),\n",
        "    edge_inputs=edge_rep.total_dim(),\n",
        "    vertex_outputs=hidden_vertex_rep_3.total_dim(),\n",
        "    edge_outputs=edge_rep.total_dim(),\n",
        "    inter_vars=hidden_message_rep_2.total_dim(),\n",
        "    inter_invt_funs=message_invariants_3,\n",
        "    vertex_invt_funs=vertex_invariants_3,\n",
        "    edge_invt_funs=edge_invariants_3,\n",
        "    inter_activation=torch.nn.ReLU(),\n",
        "    vertex_activation=torch.nn.ReLU(),\n",
        "    edge_activation=torch.nn.ReLU(),\n",
        "    is_affine=True\n",
        ")\n",
        "\n",
        "# Layer 4\n",
        "message_invariants_4 = generate_En_message_invariants(\n",
        "    dim = dim,\n",
        "    vertex_rep = hidden_vertex_rep_3,\n",
        "    edge_rep = edge_rep,\n",
        "    out_rep = hidden_vertex_rep_3\n",
        ")\n",
        "vertex_invariants_4 = generate_En_vertex_edge_invariants(\n",
        "    dim = dim,\n",
        "    input_rep = hidden_vertex_rep_3,\n",
        "    internal_rep = hidden_message_rep_3,\n",
        "    out_rep = output_vertex_rep\n",
        ")\n",
        "edge_invariants_4 = generate_En_vertex_edge_invariants(\n",
        "    dim = dim,\n",
        "    input_rep = edge_rep,\n",
        "    internal_rep = hidden_message_rep_3,\n",
        "    out_rep = edge_rep\n",
        ")\n",
        "\n",
        "layer4 = EGCL(\n",
        "    num_vertices = num_vertices,\n",
        "    adj_matrix = adj_matrix,\n",
        "    vertex_inputs=hidden_vertex_rep_3.total_dim(),\n",
        "    edge_inputs=edge_rep.total_dim(),\n",
        "    vertex_outputs=output_vertex_rep.total_dim(),\n",
        "    edge_outputs=edge_rep.total_dim(),\n",
        "    inter_vars=hidden_message_rep_3.total_dim(),\n",
        "    inter_invt_funs=message_invariants_4,\n",
        "    vertex_invt_funs=vertex_invariants_4,\n",
        "    edge_invt_funs=edge_invariants_4,\n",
        "    inter_activation=torch.nn.ReLU(),\n",
        "    vertex_activation=torch.nn.ReLU(),\n",
        "    edge_activation=torch.nn.ReLU(),\n",
        "    is_affine=True\n",
        ")"
      ],
      "metadata": {
        "id": "jHiJDlleJeXy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnEquivariantNet(nn.Module):\n",
        "\n",
        "    def __init__(self, layers: List[EGCL]):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            layers: List of EGCL layers to be applied in sequence\n",
        "        \"\"\"\n",
        "        super(EnEquivariantNet, self).__init__()\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "        # Verify layers are compatible\n",
        "        for i in range(len(layers)-1):\n",
        "            if layers[i].vertex_outputs != layers[i+1].vertex_inputs:\n",
        "                raise ValueError(f\"Layer {i} output dimension {layers[i].vertex_outputs} \"\n",
        "                               f\"doesn't match layer {i+1} input dimension {layers[i+1].vertex_inputs}\")\n",
        "            if layers[i].edge_outputs != layers[i+1].edge_inputs:\n",
        "                raise ValueError(f\"Layer {i} edge output dimension {layers[i].edge_outputs} \"\n",
        "                               f\"doesn't match layer {i+1} edge input dimension {layers[i+1].edge_inputs}\")\n",
        "\n",
        "    def forward(self, h_graph: torch.Tensor, a_graph: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            h_graph: Input vertex features\n",
        "            a_graph: Input edge features\n",
        "\n",
        "        Returns:\n",
        "            tuple(torch.Tensor, torch.Tensor): Final vertex and edge features\n",
        "        \"\"\"\n",
        "        h, a = h_graph, a_graph\n",
        "        for layer in self.layers:\n",
        "            h, a = layer(h, a)\n",
        "        return h, a\n",
        "\n",
        "# Combine layers into model\n",
        "model = EnEquivariantNet([layer1, layer2, layer3, layer4])\n",
        "\n",
        "# Create some example input data\n",
        "h_input = torch.randn(num_vertices, input_vertex_rep.total_dim())\n",
        "a_input = torch.randn(num_vertices, num_vertices, edge_rep.total_dim())\n",
        "\n",
        "# Forward pass through model\n",
        "h_output, a_output = model(h_input, a_input)\n",
        "\n",
        "print(f\"Input vertex features shape: {h_input.shape}\")\n",
        "print(f\"Output vertex features shape: {h_output.shape}\")\n",
        "print(f\"Input edge features shape: {a_input.shape}\")\n",
        "print(f\"Output edge features shape: {a_output.shape}\")"
      ],
      "metadata": {
        "id": "wgFafVIiOwn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69080e56-3be4-49ca-cfcf-104353eae0ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing messages for 13 active edges...\n",
            "Processing edge 1/13 (0->1)\n",
            "Computing 90 invariant functions...\n",
            "Computing 90 invariant functions...\n",
            "Computing 90 invariant functions...\n",
            "Computing 90 invariant functions...\n",
            "Computing 90 invariant functions...\n",
            "Processing edge 6/13 (1->3)\n",
            "Computing 90 invariant functions...\n",
            "Computing 90 invariant functions...\n",
            "Computing 90 invariant functions...\n",
            "Computing 90 invariant functions...\n",
            "Computing 90 invariant functions...\n",
            "Processing edge 11/13 (3->1)\n",
            "Computing 90 invariant functions...\n",
            "Computing 90 invariant functions...\n",
            "Computing 90 invariant functions...\n",
            "Computing message sums...\n",
            "Updating vertices...\n",
            "Vertex 1/4\n",
            "Vertex 2/4\n",
            "Vertex 3/4\n",
            "Vertex 4/4\n",
            "Updating edges...\n",
            "Edge 1/13\n",
            "Edge 6/13\n",
            "Edge 11/13\n",
            "Forward pass complete\n",
            "Computing messages for 13 active edges...\n",
            "Processing edge 1/13 (0->1)\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Processing edge 6/13 (1->3)\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Processing edge 11/13 (3->1)\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing message sums...\n",
            "Updating vertices...\n",
            "Vertex 1/4\n",
            "Vertex 2/4\n",
            "Vertex 3/4\n",
            "Vertex 4/4\n",
            "Updating edges...\n",
            "Edge 1/13\n",
            "Edge 6/13\n",
            "Edge 11/13\n",
            "Forward pass complete\n",
            "Computing messages for 13 active edges...\n",
            "Processing edge 1/13 (0->1)\n",
            "Computing 1548 invariant functions...\n",
            "Computing 1548 invariant functions...\n",
            "Computing 1548 invariant functions...\n",
            "Computing 1548 invariant functions...\n",
            "Computing 1548 invariant functions...\n",
            "Processing edge 6/13 (1->3)\n",
            "Computing 1548 invariant functions...\n",
            "Computing 1548 invariant functions...\n",
            "Computing 1548 invariant functions...\n",
            "Computing 1548 invariant functions...\n",
            "Computing 1548 invariant functions...\n",
            "Processing edge 11/13 (3->1)\n",
            "Computing 1548 invariant functions...\n",
            "Computing 1548 invariant functions...\n",
            "Computing 1548 invariant functions...\n",
            "Computing message sums...\n",
            "Updating vertices...\n",
            "Vertex 1/4\n",
            "Vertex 2/4\n",
            "Vertex 3/4\n",
            "Vertex 4/4\n",
            "Updating edges...\n",
            "Edge 1/13\n",
            "Edge 6/13\n",
            "Edge 11/13\n",
            "Forward pass complete\n",
            "Computing messages for 13 active edges...\n",
            "Processing edge 1/13 (0->1)\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Processing edge 6/13 (1->3)\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Processing edge 11/13 (3->1)\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing 336 invariant functions...\n",
            "Computing message sums...\n",
            "Updating vertices...\n",
            "Vertex 1/4\n",
            "Vertex 2/4\n",
            "Vertex 3/4\n",
            "Vertex 4/4\n",
            "Updating edges...\n",
            "Edge 1/13\n",
            "Edge 6/13\n",
            "Edge 11/13\n",
            "Forward pass complete\n",
            "Input vertex features shape: torch.Size([4, 6])\n",
            "Output vertex features shape: torch.Size([4, 6])\n",
            "Input edge features shape: torch.Size([4, 4, 1])\n",
            "Output edge features shape: torch.Size([4, 4, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now try training the model on the GM9 dataset."
      ],
      "metadata": {
        "id": "SszZhWscp2qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install PyTorch Geometric (PyG)\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8VoxU2Cq6m9",
        "outputId": "44d62f53-f106-4292-d682-9d3b5c743b19"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/108.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 423, in run\n",
            "    _, build_failures = build(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/wheel_builder.py\", line 319, in build\n",
            "    wheel_file = _build_one(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/wheel_builder.py\", line 193, in _build_one\n",
            "    wheel_path = _build_one_inside_env(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/wheel_builder.py\", line 240, in _build_one_inside_env\n",
            "    wheel_path = build_wheel_legacy(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/build/wheel_legacy.py\", line 83, in build_wheel_legacy\n",
            "    output = call_subprocess(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/subprocess.py\", line 151, in call_subprocess\n",
            "    line: str = proc.stdout.readline()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1465, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.10/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1218, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 686, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 636, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 119, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 502, in __init__\n",
            "    self.stack = StackSummary.extract(\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 383, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 306, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 30, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 396, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 365, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 323, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 11, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
            "    from pip._internal.build_env import get_runnable_pip\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/build_env.py\", line 19, in <module>\n",
            "    from pip._internal.cli.spinners import open_spinner\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n",
            "    from pip._internal.utils.logging import get_indentation\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 13, in <module>\n",
            "    from pip._vendor.rich.console import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 51, in <module>\n",
            "    from ._log_render import FormatTimeCallable, LogRender\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/_log_render.py\", line 5, in <module>\n",
            "    from .text import Text, TextType\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/text.py\", line 24, in <module>\n",
            "    from .control import strip_control_codes\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/control.py\", line 34, in <module>\n",
            "    CONTROL_CODES_FORMAT: Dict[int, Callable[..., str]] = {\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 309, in inner\n",
            "    return cached(*args, **kwds)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 1145, in __getitem__\n",
            "    return self.copy_with(params)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 1148, in copy_with\n",
            "    return _GenericAlias(self.__origin__, params,\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 1019, in __init__\n",
            "    super().__init__(origin, inst=inst, name=name)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 948, in __init__\n",
            "    self._inst = inst\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 989, in __setattr__\n",
            "    super().__setattr__(attr, val)\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "class MD17Dataset:\n",
        "    def __init__():\n",
        ""
      ],
      "metadata": {
        "id": "es_c4q0fp6vu"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}